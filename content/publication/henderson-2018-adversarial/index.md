---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Adversarial Gain
subtitle: ''
summary: ''
authors:
- Peter Henderson
- Koustuv Sinha
- Rosemary Nan Ke
- Joelle Pineau
tags: []
categories: []
date: '2018-01-01'
lastmod: 2022-07-18T19:57:58-04:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2022-07-18T23:57:58.371604Z'
publication_types:
abstract: 'Adversarial examples can be defined as inputs to a model which induce a
  mistake - where the model output is different than that of an oracle, perhaps in
  surprising or malicious ways. Original models of adversarial attacks are primarily
  studied in the context of classification and computer vision tasks. While several
  attacks have been proposed in natural language processing (NLP) settings, they often
  vary in defining the parameters of an attack and what a successful attack would
  look like. The goal of this work is to propose a unifying model of adversarial examples
  suitable for NLP tasks in both generative and classification settings. We define
  the notion of adversarial gain: based in control theory, it is a measure of the
  change in the output of a system relative to the perturbation of the input (caused
  by the so-called adversary) presented to the learner. This definition, as we show,
  can be used under different feature spaces and distance conditions to determine
  attack or defense effectiveness across different intuitive manifolds. This notion
  of adversarial gain not only provides a useful way for evaluating adversaries and
  defenses, but can act as a building block for future work in robustness under adversaries
  due to its rooted nature in stability and manifold theory. '
publication: '*Arxiv Pre-print*'
---
