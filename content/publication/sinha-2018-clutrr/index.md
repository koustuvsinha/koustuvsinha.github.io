---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Compositional Language Understanding with Text-based Relational Reasoning
subtitle: ''
summary: ''
authors:
- Koustuv Sinha
- Shagun Sodhani
- William L. Hamilton
- Joelle Pineau
tags: []
categories: []
date: '2018-01-01'
lastmod: 2022-07-18T19:57:58-04:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2022-07-18T23:57:58.806035Z'
publication_types:
abstract: 'Neural networks for natural language reasoning have largely focused on
  extractive, fact-based question-answering (QA) and common-sense inference. However,
  it is also crucial to understand the extent to which neural networks can perform
  relational reasoning and combinatorial generalization from natural language---abilities
  that are often obscured by annotation artifacts and the dominance of language modeling
  in standard QA benchmarks. In this work, we present a novel benchmark dataset for
  language understanding that isolates performance on relational reasoning. We also
  present a neural message-passing baseline and show that this model, which incorporates
  a relational inductive bias, is superior at combinatorial generalization compared
  to a traditional recurrent neural network approach. '
publication: '*Relational Representation Learning Workshop, NIPS*'
links:
- name: Code
  url: https://github.com/koustuvsinha/clutrr
---
