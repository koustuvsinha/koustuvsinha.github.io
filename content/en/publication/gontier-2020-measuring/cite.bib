@article{gontier2020measuring,
 abstract = {We are interested in understanding how well Transformer
language models (TLMs) can perform reasoning tasks when
trained on knowledge encoded in the form of natural language.
We investigate systematic generalization abilities on an
inductive logical reasoning task in natural language, which
involves reasoning over relationships between entities
grounded in first-order logical proofs. Specifically, we
perform soft theorem-proving by leveraging TLMs to generate
logical proofs represented in natural language. We
systematically test proof generation capabilities, along with
inference capabilities leveraging the generated proofs. We
observe length-generalization issues in proof generation and
inference when evaluated on longer-than-trained sequences.
However, we observe TLMs improve their generalization
performance after being exposed to longer, exhaustive proofs.
In addition, we discover that TLMs are able to generalize
better using backward-chaining proofs compared to their
forward-chaining counterparts, while they find it easier to
generate forward chaining proofs. We observe that models that
are not trained to generate proofs are better at generalizing
to problems based on longer proofs. This result suggests that
Transformers have efficient, yet not interpretable reasoning
strategies internally. These results also highlight the
systematic generalization issues in TLMs in the context of
logical reasoning, and we believe this work will motivate
deeper inspection of their underlying reasoning strategies. },
 arxiv = {2009.14786},
 author = {Nicolas Gontier and Koustuv Sinha and Siva Reddy and
Christopher Pal},
 code = {https://github.com/NicolasAG/SGinPG},
 eprint = {2009.14786},
 journal = {Neural Information Procesing Systems (NeurIPS)},
 title = {Measuring Systematic Generalization in Neural Proof
Generation with Transformers},
 url = {https://arxiv.org/abs/2009.14786},
 year = {2020}
}

