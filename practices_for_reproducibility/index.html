<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Fira+Sans:wght@400;700&family=Roboto+Mono&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Fira+Sans:wght@400;700&family=Roboto+Mono&display=swap" media=print onload='this.media="all"'><meta name=author content="Koustuv Sinha"><meta name=description content="A recurrent challenge in machine learning research is to ensure that the presented and published results are reliable, robust, and reproducible [4,5,6,7].
Reproducibility, obtaining similar results as presented in a paper using the same code and data, is necessary to verify the reliability of research findings."><link rel=alternate hreflang=en-us href=/practices_for_reproducibility/><meta name=theme-color content="#1565c0"><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.99ecbdcb5f73ec1ef08ee51f9bf420b0.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png><link rel=canonical href=/practices_for_reproducibility/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@koustuvsinha"><meta property="twitter:creator" content="@koustuvsinha"><meta property="og:site_name" content="Koustuv Sinha"><meta property="og:url" content="/practices_for_reproducibility/"><meta property="og:title" content="ML Reproducibility Tools and Best Practices | Koustuv Sinha"><meta property="og:description" content="A recurrent challenge in machine learning research is to ensure that the presented and published results are reliable, robust, and reproducible [4,5,6,7].
Reproducibility, obtaining similar results as presented in a paper using the same code and data, is necessary to verify the reliability of research findings."><meta property="og:image" content="/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2020-08-05T00:00:00+00:00"><meta property="article:modified_time" content="2020-08-05T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"/practices_for_reproducibility/"},"headline":"ML Reproducibility Tools and Best Practices","datePublished":"2020-08-05T00:00:00Z","dateModified":"2020-08-05T00:00:00Z","author":{"@type":"Person","name":"Koustuv Sinha"},"publisher":{"@type":"Organization","name":"Koustuv Sinha","logo":{"@type":"ImageObject","url":"/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_3.png"}},"description":"A recurrent challenge in machine learning research is to ensure that the presented and published results are reliable, robust, and reproducible [4,5,6,7].\nReproducibility, obtaining similar results as presented in a paper using the same code and data, is necessary to verify the reliability of research findings."}</script><title>ML Reproducibility Tools and Best Practices | Koustuv Sinha</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=030cc810f244398b11ac834b8ae7cb3f><script src=/js/wowchemy-init.min.df5b75624ac8e15e4f78f4316a963728.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Koustuv Sinha</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Koustuv Sinha</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/post><span>Blog</span></a></li><li class=nav-item><a class=nav-link href=/activities><span>Activities</span></a></li><li class=nav-item><a class=nav-link href=/project><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/publication><span>Publications</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><article class="article article-project"><div class="article-container pt-3"><h1>ML Reproducibility Tools and Best Practices</h1><div class=article-metadata><div><span>Koustuv Sinha</span>, <span>Jessica Zosa Forde</span></div><span class=article-date>Aug 5, 2020</span>
<span class=middot-divider></span>
<span class=article-reading-time>12 min read</span></div></div><div class=article-container><div class=article-style><p>A recurrent challenge in machine learning research is to ensure that the
presented and published results are reliable, robust, and reproducible
[<a href=http://proceedings.mlr.press/v97/bouthillier19a.html target=_blank rel=noopener>4</a>,<a href=https://arxiv.org/abs/1711.10337 target=_blank rel=noopener>5</a>,<a href=https://arxiv.org/abs/1709.06560 target=_blank rel=noopener>6</a>,<a href=https://arxiv.org/abs/1909.06674 target=_blank rel=noopener>7</a>].</p><p>Reproducibility, obtaining similar results as presented in a paper using
the same code and data, is necessary to verify the reliability of
research findings. Reproducibility is also an important step to promote
open and accessible research, thereby allowing the scientific community
to quickly integrate new findings and convert ideas to practice.
Reproducibility also promotes the use of robust experimental workflows,
which potentially reduce unintentional errors.</p><p>In this blog post, we will share commonly used tools and explain 12
basic practices that you can use in your research to ensure reproducible
science.</p><h2 id=tools>Tools</h2><p><strong>Updated</strong> : 21st December, 2020</p><table><thead><tr><th></th><th>Practice</th><th>Tools</th></tr></thead><tbody><tr><td>1</td><td>Config Management</td><td><a href=https://hydra.cc target=_blank rel=noopener>Hydra</a>, <a href=https://github.com/omry/omegaconf target=_blank rel=noopener>OmegaConf</a>, <a href=https://github.com/PyTorchLightning/pytorch-lightning target=_blank rel=noopener>Pytorch Lightning</a></td></tr><tr><td>2</td><td>Checkpoint Management</td><td><a href=https://github.com/PyTorchLightning/pytorch-lightning target=_blank rel=noopener>Pytorch Lightning</a>, <a href=https://github.com/williamFalcon/test-tube target=_blank rel=noopener>TestTube</a></td></tr><tr><td>3</td><td>Logging</td><td><a href=https://www.tensorflow.org/tensorboard target=_blank rel=noopener>Tensorboard</a>, <a href=https://www.comet.ml/site/ target=_blank rel=noopener>Comet.ML</a>, <a href=https://www.wandb.com/ target=_blank rel=noopener>Weights & Biases</a>, <a href=https://mlflow.org/ target=_blank rel=noopener>MLFlow</a>, <a href=https://github.com/facebookresearch/visdom target=_blank rel=noopener>Visdom</a>, <a href=https://neptune.ai/ target=_blank rel=noopener>Neptune</a></td></tr><tr><td>4</td><td>Seed</td><td><em>Check best practices below</em></td></tr><tr><td>-</td><td>Experiment Management</td><td><a href=https://github.com/PyTorchLightning/pytorch-lightning target=_blank rel=noopener>Pytorch Lightning</a>, <a href=https://mlflow.org target=_blank rel=noopener>MLFlow</a>, <a href=https://determined.ai/ target=_blank rel=noopener>Determined.AI</a></td></tr><tr><td>5</td><td>Versioning</td><td><a href=https://github.com target=_blank rel=noopener>Github</a>, <a href=https://gitlab.com target=_blank rel=noopener>Gitlab</a>, <a href=https://replicate.ai/ target=_blank rel=noopener>Replicate.AI</a></td></tr><tr><td>6</td><td>Data Management</td><td><a href=https://dvc.org target=_blank rel=noopener>DVC</a>, <a href=https://cml.dev target=_blank rel=noopener>CML</a>, <a href=https://replicate.ai/ target=_blank rel=noopener>Replicate.AI</a></td></tr><tr><td>7</td><td>Data analysis</td><td><a href=https://jupyter.org/ target=_blank rel=noopener>Jupyter Notebook</a>, <a href=https://papermill.readthedocs.io/en/latest/ target=_blank rel=noopener>papermill</a>, <a href=https://jupyterlab.readthedocs.io/en/stable/ target=_blank rel=noopener>JupyterLab</a>, <a href=https://colab.research.google.com/ target=_blank rel=noopener>Google Colab</a></td></tr><tr><td>8</td><td>Reporting</td><td><a href=https://matplotlib.org/ target=_blank rel=noopener>Matplotlib</a>, <a href=https://seaborn.pydata.org/ target=_blank rel=noopener>Seaborn</a> , <a href=https://pandas.pydata.org/ target=_blank rel=noopener>Pandas</a>, <a href=https://www.overleaf.com/ target=_blank rel=noopener>Overleaf</a></td></tr><tr><td>9</td><td>Dependency Management</td><td><a href=https://pypi.org/project/pip/ target=_blank rel=noopener>pip</a>, <a href=https://docs.conda.io/en/latest/ target=_blank rel=noopener>conda</a>, <a href=https://python-poetry.org/ target=_blank rel=noopener>Poetry</a>, <a href=https://www.docker.com/ target=_blank rel=noopener>Docker</a>, <a href=https://sylabs.io/docs/ target=_blank rel=noopener>Singularity</a>, <a href=https://github.com/jupyter/repo2docker target=_blank rel=noopener>repo2docker</a></td></tr><tr><td>10</td><td>Open Source Release</td><td><a href=https://stackoverflow.com/questions/5189560/squash-my-last-x-commits-together-using-git target=_blank rel=noopener>Squash Commits</a>, <a href=https://mybinder.org/ target=_blank rel=noopener>Binder</a></td></tr><tr><td>11</td><td>Effective Communication</td><td><a href=https://medium.com/paperswithcode/ml-code-completeness-checklist-e9127b168501 target=_blank rel=noopener>ML Code Completeness Checklist</a>, <a href=https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf target=_blank rel=noopener>ML Reproducibility Checklist</a></td></tr><tr><td>12</td><td>Test and Validate</td><td><a href=https://aws.amazon.com/ target=_blank rel=noopener>AWS</a>, <a href=https://cloud.google.com/ target=_blank rel=noopener>GCP</a>, <a href=https://codeocean.com/ target=_blank rel=noopener>CodeOcean</a></td></tr></tbody></table><h2 id=practices>Practices</h2><h3 id=config-management>1. Config Management</h3><p>When you begin implementing your research code, the first line of work
is to define an argument parser to define the set of parameters your
code expects. These set of hyperparameters can typically look like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>python train.py --hidden_dim <span class=m>100</span> --batch_size <span class=m>32</span> --num_tasks <span class=m>10</span> --dropout 0.2 --with_mask --log_interval <span class=m>100</span> --learning_rate 0.001 --optimizer sgd --scheduler plateau --scheduler_gamma 0.9 --weight_decay 0.9
</span></span></code></pre></div><p>These sets of arguments typically grow over time in your research
project, making maintenance and reproducibility a pain. Typically in
your code, you should be careful to log all hyperparameters for all
experiments, so that you can replicate an old version of your code.
<a href=https://github.com/PyTorchLightning/pytorch-lightning target=_blank rel=noopener>Pytorch
Lightning</a> provides a great way to log all hyperparameters in <code>.csv</code>
files in the experiment output folder, allowing for better
reproducibility.</p><p>An alternative to using a long list of argparse elements is to use
config files. Config files can be either in JSON or YAML format (I
prefer YAML due to the ability to add comments), where you can set your
hyperparams in a logically nested way. The above set of hyperparams
could be organized as:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=c># config.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>general</span><span class=p>:</span><span class=w> </span><span class=c># for generic args</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>batch_size</span><span class=p>:</span><span class=w> </span><span class=m>32</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>num_tasks</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>with_mask</span><span class=p>:</span><span class=w> </span><span class=kc>False</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>log_interval</span><span class=p>:</span><span class=w> </span><span class=m>100</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>optim</span><span class=p>:</span><span class=w> </span><span class=c># for optimizer args</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>learning_rate</span><span class=p>:</span><span class=w> </span><span class=m>0.001</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>optimizer</span><span class=p>:</span><span class=w> </span><span class=l>sgd</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>scheduler</span><span class=p>:</span><span class=w> </span><span class=l>plateau</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>scheduler_gamma</span><span class=p>:</span><span class=w> </span><span class=m>0.9</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>weight_decay</span><span class=p>:</span><span class=w> </span><span class=m>0.9</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>model</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>hidden_dim</span><span class=p>:</span><span class=w> </span><span class=m>100</span><span class=w>
</span></span></span></code></pre></div><p><a href=https://github.com/omry/omegaconf target=_blank rel=noopener>OmegaConf</a> (part of
<a href=https://hydra.cc target=_blank rel=noopener>Hydra</a>) is a great library that allows you to
maintain these config files while providing added flexibility to import
previous config files and modify only a few values.</p><h3 id=checkpoint-management>2. Checkpoint Management</h3><p>Managing your model checkpoints is very important in terms of
reproducibility, as it allows you to release trained models for the
community to easily verify your work, as well as build upon it. Ideally,
you should save your checkpoints as frequently as possible. Given the
system resource restrictions, it is usually not feasible. Thus, it is
ideal to save the last checkpoint along with the checkpoint of the <em>last
best model</em> (according to your evaluation metrics).
<a href=https://github.com/PyTorchLightning/pytorch-lightning target=_blank rel=noopener>Pytorch
Lightning</a> provides an in-built solution to do this efficiently.</p><h3 id=logging>3. Logging</h3><p>When training your model, you realize that for several parameters it is
not giving you the ideal performance. Ideally, you want to check several
things. Is the training loss of the model saturating? Is it still going
down? How is the validation performance over training look like? You
need to log all the metrics efficiently, and later plot those metrics in
nice shiny plots for analysis and inspection.</p><p>Logging is also important for reproducibility, so researchers can verify
the training progression of their replications in great detail.</p><p>In the bare-bones setup, you could just log all metrics in the
filesystem and then plot by loading them in a python script using
matplotlib. To make this process easy and also to provide live,
interactive plots, several services are available now which you can
leverage in your work.
<a href=https://www.tensorflow.org/tensorboard target=_blank rel=noopener>Tensorboard</a>, for example, is
popular in the ML community primarily for its early adoption and ability
to deploy locally. Newer entrants, like
<a href=https://www.comet.ml/site/ target=_blank rel=noopener>Comet.ML</a>,
<a href=https://www.wandb.com/ target=_blank rel=noopener>WandB</a> or <a href=https://mlflow.org/ target=_blank rel=noopener>MLFlow</a>,
provide exciting features ranging from sharable online logging
interfaces, with fine-grained ability to monitor experiments and
hyperparams. In a future blog post, we will discuss on the pros and cons
of these systems.</p><h3 id=setting-the-seed>4. Setting the seed</h3><p>Probably the most important aspect of the exact reproducibility of your
research is the seed of the experiment. Although exact reproducibility
is not guaranteed, especially in GPU execution environments
[<a href=https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#reproducibility target=_blank rel=noopener>2</a>,
<a href=https://pytorch.org/docs/stable/notes/randomness.html target=_blank rel=noopener>8</a>], it&rsquo;s
still beneficial to report the seed due to its impact on your results.</p><p>When you begin your experiments, it suggested to first set the seed
using scripts like these (assuming if you use PyTorch):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>set_seed</span><span class=p>(</span><span class=n>seed</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Set seed&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>manual_seed_all</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>torch</span><span class=o>.</span><span class=n>backends</span><span class=o>.</span><span class=n>cudnn</span><span class=o>.</span><span class=n>deterministic</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>        <span class=n>torch</span><span class=o>.</span><span class=n>backends</span><span class=o>.</span><span class=n>cudnn</span><span class=o>.</span><span class=n>benchmark</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>    <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s2>&#34;PYTHONHASHSEED&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=nb>str</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Do not optimize the seed like a hyperparameter. If your algorithm only
works on a range of seeds, it&rsquo;s not a robust contribution.</strong></p><p>Reporting the performance of your model on <em>multiple seeds</em> captures the
variance of the proposed model. Before beginning your experiments,
randomly draw \(n\) seeds and set them aside in your config file, and
report all experimental results aggregated over those \(n\) seeds.
\(n=5\) is a good starting point, but you an always increase this
number.</p><h3 id=version-control>5. Version Control</h3><p>To track your research effectively, we highly recommended practice
setting up version control using <code>git</code> in your repository from the
get-go. You can use a service like <a href=https://github.com target=_blank rel=noopener>Github</a> or
<a href=https://gitlab.com/ target=_blank rel=noopener>Gitlab</a> as your hosting provider.</p><p>Use <code>git commit=s to explain to your future self (and your collaborators) what change you made to your experiment at a given time. Ideally, you should /always commit before you run an experiment/, so that you can =tag</code> the results with specific commits. Be as detailed
with your commit messages as you can - your future self will thank you!</p><p>Check out the
<a href=https://github.com/huggingface/transformers/commit/9996f697e3ed7a0d6fe4348953723ad6b9d51477 target=_blank rel=noopener>commits</a>
from
<a href=https://github.com/huggingface/transformers target=_blank rel=noopener>Huggingface/transformers</a>
repository for a nice example.</p><h3 id=data-management>6. Data Management</h3><p>Managing your data is extremely important for reproducibility,
especially when you propose a new dataset or a new dataset split. In
your many rounds of experiments, you would probably work with different
splits of the data, hence tracking all those changes should have similar
priority as tracking your code.</p><p>The easiest way to track your data is to add it to the git version
system or use cloud storage solutions such as Google Drive, AWS S3 to
store your datasets.</p><p>For large datasets, you can also use
<a href=https://git-lfs.github.com/ target=_blank rel=noopener><code>git-lfs</code></a>, or maintain a md5 hash of
the dataset in your config file, like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>md5_update_from_dir</span><span class=p>(</span><span class=n>directory</span><span class=p>:</span> <span class=n>Union</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Path</span><span class=p>],</span> <span class=nb>hash</span><span class=p>:</span> <span class=n>Hash</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Hash</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>Path</span><span class=p>(</span><span class=n>directory</span><span class=p>)</span><span class=o>.</span><span class=n>is_dir</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>path</span> <span class=ow>in</span> <span class=nb>sorted</span><span class=p>(</span><span class=n>Path</span><span class=p>(</span><span class=n>directory</span><span class=p>)</span><span class=o>.</span><span class=n>iterdir</span><span class=p>(),</span> <span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>p</span><span class=p>:</span> <span class=nb>str</span><span class=p>(</span><span class=n>p</span><span class=p>)</span><span class=o>.</span><span class=n>lower</span><span class=p>()):</span>
</span></span><span class=line><span class=cl>        <span class=nb>hash</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=n>path</span><span class=o>.</span><span class=n>name</span><span class=o>.</span><span class=n>encode</span><span class=p>())</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>path</span><span class=o>.</span><span class=n>is_file</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=nb>hash</span> <span class=o>=</span> <span class=n>md5_update_from_file</span><span class=p>(</span><span class=n>path</span><span class=p>,</span> <span class=nb>hash</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>path</span><span class=o>.</span><span class=n>is_dir</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=nb>hash</span> <span class=o>=</span> <span class=n>md5_update_from_dir</span><span class=p>(</span><span class=n>path</span><span class=p>,</span> <span class=nb>hash</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>hash</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>md5_dir</span><span class=p>(</span><span class=n>directory</span><span class=p>:</span> <span class=n>Union</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Path</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>str</span><span class=p>(</span><span class=n>md5_update_from_dir</span><span class=p>(</span><span class=n>directory</span><span class=p>,</span> <span class=n>hashlib</span><span class=o>.</span><span class=n>md5</span><span class=p>())</span><span class=o>.</span><span class=n>hexdigest</span><span class=p>())</span>
</span></span></code></pre></div><p><a href=https://stackoverflow.com/a/54477583 target=_blank rel=noopener>Source - StackOverflow</a></p><p>Having such a hash will allow you to track which dataset or data split
you were working on at a certain commit.</p><h3 id=data-analysis>7. Data Analysis</h3><p>Keeping track of the analysis you perform on the data/results is also
very important in terms of the reproducibility of your contribution.
<a href=https://jupyter.org target=_blank rel=noopener>Jupyter Notebooks</a> are the standard in
maintaining all your analysis and plotting functions in one place.
Ideally, you should separate notebooks for data analysis, result
analysis, plot generation, and table generation, and add them in your
version control. Pandas'
<a href=https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_latex.html target=_blank rel=noopener>to_latex</a>
allows you to directly write your results as a latex table, removing
error-prone copying of results into LaTeX.</p><p>When you need to update the results in your paper, you can just access
the corresponding file and re-run the cells. You can also parameterize
and run notebooks with the
<a href=https://github.com/nteract/papermill#execute-via-the-python-api target=_blank rel=noopener>papermill
API</a> so that your notebooks are cleanly executed your desired analysis
parameters.</p><p>Maintaining Jupyter Notebooks can get tricky over time. Consider
following the best practices [<a href=https://arxiv.org/abs/1810.08055 target=_blank rel=noopener>1</a>]
and use
<a href=https://github.com/ipython-contrib/jupyter_contrib_nbextensions target=_blank rel=noopener>Jupter
contrib nbextensions</a> to supercharge your notebooks!</p><h3 id=reporting-results>8. Reporting Results</h3><p>When reporting your results, it is ideal to run your experiments in
different seeds and/or different datasets. Thus, your results should
contain plots with error bars and tables with standard deviations. You
should also describe how the descriptive statistics were calculated,
e.g. mean reward over multiple seeds. Statistical testing and
highlighting statistically significant values is also encouraged
[<a href=https://arxiv.org/abs/1904.10922 target=_blank rel=noopener>9</a>]. This information provides a
more realistic assessment of the performance of a model and avoids the
sharing of overly optimistic results
[<a href=http://proceedings.mlr.press/v97/bouthillier19a.html target=_blank rel=noopener>4</a>,<a href=https://arxiv.org/abs/1711.10337 target=_blank rel=noopener>5</a>,<a href=https://arxiv.org/abs/1709.06560 target=_blank rel=noopener>6</a>,<a href=https://arxiv.org/abs/1909.06674 target=_blank rel=noopener>7</a>].</p><p>A higher bar of reproducibility is to report the results on <em>multiple
datasets</em> to highlight the robustness of your model. Even if the model
has larger variance over different datasets, its still encouraged to
report them all - to avoid the discovery of these issues later on.</p><p>While reporting your results, consult the
<a href=https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf target=_blank rel=noopener>ML
Reproducibility Checklist</a> which has detailed guidelines on the best
practices for reporting figures and tables.</p><h3 id=managing-dependencies>9. Managing Dependencies</h3><p>Irreproducibility often stems from software deprecation. To replicate a
published work, the first thing to do is to match the same development
environment, containing the same libraries that the program expects.
Thus, it is crucial to document the libraries and their versions that
you use in your experiments. After your experiments are stable, you can
leverage <code>pip</code> or <code>conda</code> to collect all libraries that have been used:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>$ pip freeze &gt; requirements.txt
</span></span><span class=line><span class=cl>$ conda env <span class=nb>export</span> &gt; environment.yml
</span></span></code></pre></div><p>You can also leverage headless virtual machines such as
<a href=https://www.docker.com/ target=_blank rel=noopener>Docker</a> or
<a href=https://sylabs.io/docs/ target=_blank rel=noopener>Singularity</a> to provide the exact
reproducible dev environment used for the experiments. Singularity, in
particular, is supported in many HPC systems (such as
<a href=https://www.computecanada.ca/ target=_blank rel=noopener>Compute Canada</a>), which can be used to
train and then subsequently release your experiments to the public. You
can also convert your existing repository into a Docker environment
using <a href=https://github.com/jupyter/repo2docker target=_blank rel=noopener>repo2docker</a>.</p><h3 id=open-source-release>10. Open Source Release</h3><p>After you have published your paper, consider open sourcing your
experiments. This not only encourages reproducible research but also
adds more visibility to your paper. Once you release your code, consider
adding it to <a href=https://paperswithcode.com/ target=_blank rel=noopener>Papers With Code</a> for added
visibility. You can also release a demo on
<a href=https://mybinder.org target=_blank rel=noopener>Binder</a> or
<a href=https://colab.research.google.com/ target=_blank rel=noopener>Colab</a> to encourage people to use
your model.</p><p>For good examples of model demos check out
[<a href=https://distill.pub/2018/differentiable-parameterizations/ target=_blank rel=noopener>10</a>].</p><p>Before releasing your code, check the following:</p><ul><li>Squash the commits in the public branch (master) into a single commit<ul><li>Helps remove your private experiment commit messages (and the
awkward comments!)</li></ul></li><li>Make sure your code does not contain any API keys (for loggers such as
WandB or Comet.ML)</li><li>Keep an eye out for hardcoded file paths</li><li>Improve readability of your code using formatters such as
<a href=https://pypi.org/project/black/ target=_blank rel=noopener>Black</a>. Obscure, poorly written
codebases, even when they run, are oftentimes impossible to reuse or
build on top of</li><li>Document your functions and classes appropriately. In ML, it&rsquo;s
beneficial to the reader if you annotate your code with input and
output tensor dimensions.</li></ul><h3 id=effective-communication>11. Effective Communication</h3><p>When releasing your code, try to add as much information about the code
in the README file. <a href=https://paperswithcode.com/ target=_blank rel=noopener>Papers With Code</a>
released
<a href=https://medium.com/paperswithcode/ml-code-completeness-checklist-e9127b168501 target=_blank rel=noopener>ML
Code Completeness checklist</a>, which suggests adding the following in
your README:</p><ul><li>Dependency information</li><li>Training scripts</li><li>Evaluation scripts</li><li>Pre-trained models</li><li>Results</li></ul><p><a href=https://paperswithcode.com/ target=_blank rel=noopener>Papers With Code</a> evaluated repositories
released after NeurIPS 2019 and found repositories that do not address
any of the above only got a median of 1.5 Github stars, whereas
repositories which have all five of the above criteria got <strong>196.5</strong>
median stars! Only 9% of the repositories fulfilled the 5 points, so
definitely we can do better about communicating our research. The better
the communication, the better it is in terms of reproducibility.</p><p>You should always mention clearly the source of the dataset used in the
work. If you are releasing a new dataset or pretrained model for the
community, consider adding proper documentation for easy access, such as
a <a href=https://arxiv.org/abs/1803.09010 target=_blank rel=noopener>datasheet</a> or
<a href=https://arxiv.org/abs/1810.03993 target=_blank rel=noopener>model card</a>. These are READMEs for
the dataset or model which contains:</p><ul><li>Motivation</li><li>Composition</li><li>Collection Process</li><li>Preprocessing</li><li>Use cases</li><li>Distribution</li><li>Maintenance</li></ul><p>Read the papers [<a href=https://arxiv.org/abs/1803.09010 target=_blank rel=noopener>3</a>,
<a href=https://arxiv.org/abs/1810.03993 target=_blank rel=noopener>11</a>] for more details on these
questions and how to address them. You can choose to publish your
dataset either through Github repository or through
<a href=https://zenodo.org/ target=_blank rel=noopener>Zenodo</a>.</p><h3 id=test-and-validate>12. Test and Validate</h3><p>Finally, it&rsquo;s important from the reproducibility perspective to test
your implementation in a <em>different environment</em> than the training
setup. This testing doesn&rsquo;t necessarily mean you have to re-train the
entire pipeline. Specifically, you should make sure that the training
and evaluation scripts are running in the test environment.</p><p>To get an isolated test environment, you can use AWS or GCP cloud
instances. You can also checkout <a href=https://codeocean.com/ target=_blank rel=noopener>CodeOcean</a>
which provides isolated AWS instances tied to Jupyter Notebooks for easy
evaluation.</p><h2 id=final-thoughts>Final Thoughts</h2><p>Reproducibility is hard. Maintaining a reproducible research codebase is
harder when the incentive is to publish your ideas quicker than your
competitor. Nevertheless, we agree with what Joelle Pineau said in
NeurIPS 2018 :
<a href="https://www.facebook.com/watch/live/?v=2120856364798049&ref=watch_permalink" target=_blank rel=noopener><em>&ldquo;Science
is not a competitive sport&rdquo;</em></a>. We need to invest more time and care in
our research, and we need to ensure as computer scientists our work is
reproducible so that it adds value to the reader and practitioners who
would build upon our work.</p><p>We hope this post will be useful in your research. Feel free to comment
if you have any particular point/libraries that we missed, we would be
happy to add them.</p><h2 id=acknowledgements>Acknowledgements</h2><p>Many thanks to Joelle Pineau for encouraging writing this draft, and
helping formulating the best practices. Thanks to Shagun Sodhani,
Matthew Muckley and Michela Paganini for providing feedback on the
draft. Thanks to <a href=https://dl4sci-school.lbl.gov/ target=_blank rel=noopener>Deep Learning for
Science School</a> for inviting Koustuv to speak about reproducibility on
August 2020, for which this blog post is a point of reference.</p><h2 id=references>References</h2><ol><li>Rule A, Birmingham A, Zuniga C, Altintas I, Huang SC, Knight R,
Moshiri N, Nguyen MH, Rosenthal SB, Pérez F, Rose PW.
<a href=https://arxiv.org/abs/1810.08055 target=_blank rel=noopener>Ten simple rules for reproducible
research in Jupyter notebooks</a>. arXiv preprint arXiv:1810.08055.
2018 Oct 13.</li><li><a href=https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#reproducibility target=_blank rel=noopener>Nvidia
CUDNN Developer Guides</a></li><li>Gebru T, Morgenstern J, Vecchione B, Vaughan JW, Wallach H, Daumé III
H, Crawford K. <a href=https://arxiv.org/abs/1803.09010 target=_blank rel=noopener>Datasheets for
datasets</a>. arXiv preprint arXiv:1803.09010. 2018 Mar 23.</li><li>Bouthillier X, Laurent C, Vincent P.
<a href=http://proceedings.mlr.press/v97/bouthillier19a.html target=_blank rel=noopener>Unreproducible
research is reproducible</a>. In International Conference on Machine
Learning 2019 May 24 (pp. 725-734).</li><li>Lucic M, Kurach K, Michalski M, Gelly S, Bousquet O.
<a href=https://arxiv.org/abs/1711.10337 target=_blank rel=noopener>Are GANs created equal? a
large-scale study</a>. In Advances in Neural Information Processing
Systems 2018 (pp. 700-709).</li><li>Henderson P, Islam R, Bachman P, Pineau J, Precup D, Meger D.
<a href=https://arxiv.org/abs/1709.06560 target=_blank rel=noopener>Deep Reinforcement learning that
matters</a>. In Thirty-Second AAAI Conference on Artificial
Intelligence 2018 Apr 29.</li><li>Raff E. <a href=https://arxiv.org/abs/1909.06674 target=_blank rel=noopener>A Step Toward Quantifying
Independently Reproducible Machine Learning Research</a>. In Advances
in Neural Information Processing Systems 2019 (pp. 5485-5495).</li><li><a href=https://pytorch.org/docs/stable/notes/randomness.html target=_blank rel=noopener>Pytorch note
on reproducibility</a></li><li>Forde JZ, Paganini M. <a href=https://arxiv.org/abs/1904.10922 target=_blank rel=noopener>The
Scientific Method in the Science of Machine Learning</a>. In ICLR
Debugging Machine Learning Models Workshop 2019.</li><li>Mordvintsev A, Pezzotti N, Schubert L, Olah C.
<a href=https://distill.pub/2018/differentiable-parameterizations/ target=_blank rel=noopener>Differentiable
Image Parameterizations</a>. Distill 2018.</li><li>Mitchell M, Wu S, Zaldivar A, Barnes P, Vasserman L, Hutchinson B,
Spitzer E, Raji ID, and Gebru T.
<a href=https://arxiv.org/abs/1810.03993 target=_blank rel=noopener>Model Cards for Model
Reporting</a>. In Proceedings of the Conference on Fairness,
Accountability, and Transparency (FAT* &lsquo;19). Association for
Computing Machinery, New York, NY, USA, 220&ndash;229.</li></ol></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=/practices_for_reproducibility/&text=ML%20Reproducibility%20Tools%20and%20Best%20Practices" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=/practices_for_reproducibility/&t=ML%20Reproducibility%20Tools%20and%20Best%20Practices" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=ML%20Reproducibility%20Tools%20and%20Best%20Practices&body=/practices_for_reproducibility/" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=/practices_for_reproducibility/&title=ML%20Reproducibility%20Tools%20and%20Best%20Practices" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=ML%20Reproducibility%20Tools%20and%20Best%20Practices%20/practices_for_reproducibility/" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=/practices_for_reproducibility/&title=ML%20Reproducibility%20Tools%20and%20Best%20Practices" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><section id=comments><script src=https://utteranc.es/client.js repo=koustuvsinha/koustuvsinha.github.io issue-term=pathname label=comment theme=github-light crossorigin=anonymous async></script></section><div class="project-related-pages content-widget-hr"></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2022 Koustuv Sinha. This website is built using <a href=https://ox-hugo.scripter.co/ target=_blank rel=noopener>ox-hugo</a>, <a href=https://orgmode.org/ target=_blank rel=noopener>org-mode</a>, <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> and <a href=https://wowchemy.com/ target=_blank rel=noopener>Wowchemy</a>, on Emacs 28.1. Opinions expressed in this website are solely my own. While we strive for excellence in research, it is important to remember that <a href=https://www.facebook.com/nipsfoundation/videos/2120856364798049/ target=_blank rel=noopener><em>&ldquo;Science is not a competitive sport&rdquo;</em></a>.</p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script>
<script src=/en/js/wowchemy.min.992ab4bf929c75fb2aff9ec73febac85.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>