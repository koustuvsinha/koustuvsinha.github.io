---
---

@string{aps = {American Physical Society,}}

@article{Pineau:2019,
  author          = {Pineau, Joelle and Sinha, Koustuv and Fried, Genevieve and
                  Ke, Rosemary Nan and Larochelle, Hugo},
  title           = {{ICLR Reproducibility Challenge 2019}},
  journal         = {ReScience C},
  year            = 2019,
  month           = may,
  volume          = 5,
  number          = 2,
  pages           = {{#5}},
  doi             = {10.5281/zenodo.3158244},
  url             = {https://zenodo.org/record/3158244/files/article.pdf},
  code_url        = {https://github.com/reproducibility-challenge/iclr_2019},
  review_url      = {https://github.com/ReScience/submissions/issues/5},
  type            = {Editorial},
  epdf            = {../assets/files/rc_iclr_2019.pdf},
  keywords        = {machine learning, ICLR, reproducibility challenge}
}

@article{gontier2018,
  author          = {Nicolas A. Gontier and Koustuv Sinha and Peter Henderson
                  and Iulian Serban and Michael Noseworthy and Prasanna
                  Parthasarathi and Joelle Pineau},
  title           = {The RLLChatbot: a solution to the ConvAI Challenge},
  year            = 2018,
  arxiv           = {1811.02714},
  journal         = {under review at Dialog & Discourse Journal (D&D)},
  abstract        = {Current conversational systems can follow simple commands
                  and answer basic questions, but they have difficulty
                  maintaining coherent and open-ended conversations about
                  specific topics. Competitions like the Conversational
                  Intelligence (ConvAI) challenge are being organized to push
                  the research development towards that goal. This article
                  presents in detail the RLLChatbot that participated in the
                  2017 ConvAI challenge. The goal of this research is to better
                  understand how current deep learning and reinforcement
                  learning tools can be used to build a robust yet flexible open
                  domain conversational agent. We provide a thorough description
                  of how a dialog system can be built and trained from mostly
                  public-domain datasets using an ensemble model. The first
                  contribution of this work is a detailed description and
                  analysis of different text generation models in addition to
                  novel message ranking and selection methods. Moreover, a new
                  open-source conversational dataset is presented. Training on
                  this data significantly improves the Recall@k score of the
                  ranking and selection mechanisms compared to our baseline
                  model responsible for selecting the message returned at each
                  interaction. }
}

@article{gontier2020measuring,
  title           = {Measuring Systematic Generalization in Neural Proof
                  Generation with Transformers},
  author          = {Nicolas Gontier and Koustuv Sinha and Siva Reddy and
                  Christopher Pal},
  abstract        = {We are interested in understanding how well Transformer
                  language models (TLMs) can perform reasoning tasks when
                  trained on knowledge encoded in the form of natural language.
                  We investigate systematic generalization abilities on an
                  inductive logical reasoning task in natural language, which
                  involves reasoning over relationships between entities
                  grounded in first-order logical proofs. Specifically, we
                  perform soft theorem-proving by leveraging TLMs to generate
                  logical proofs represented in natural language. We
                  systematically test proof generation capabilities, along with
                  inference capabilities leveraging the generated proofs. We
                  observe length-generalization issues in proof generation and
                  inference when evaluated on longer-than-trained sequences.
                  However, we observe TLMs improve their generalization
                  performance after being exposed to longer, exhaustive proofs.
                  In addition, we discover that TLMs are able to generalize
                  better using backward-chaining proofs compared to their
                  forward-chaining counterparts, while they find it easier to
                  generate forward chaining proofs. We observe that models that
                  are not trained to generate proofs are better at generalizing
                  to problems based on longer proofs. This result suggests that
                  Transformers have efficient, yet not interpretable reasoning
                  strategies internally. These results also highlight the
                  systematic generalization issues in TLMs in the context of
                  logical reasoning, and we believe this work will motivate
                  deeper inspection of their underlying reasoning strategies. },
  year            = 2020,
  arxiv           = {2009.14786},
  eprint          = {2009.14786},
  journal         = {Neural Information Procesing Systems (NeurIPS)},
  code            = {https://github.com/NicolasAG/SGinPG},
  url             = {https://arxiv.org/abs/2009.14786}
}

@article{goodwin2020probing,
  author          = {Emily Goodwin and Koustuv Sinha and Timothy J. O'Donnell},
  title           = {Probing Linguistic Systematicity},
  year            = 2020,
  journal         = {Association of Computational Linguistics (ACL)},
  arxiv           = {2005.04315},
  url             = {https://arxiv.org/abs/2005.04315},
  code            = {https://github.com/emilygoodwin/systematicity},
  abstract        = {Recently, there has been much interest in the question of
                  whether deep natural language understanding models exhibit
                  systematicity; generalizing such that units like words make
                  consistent contributions to the meaning of the sentences in
                  which they appear. There is accumulating evidence that neural
                  models often generalize non-systematically. We examined the
                  notion of systematicity from a linguistic perspective,
                  defining a set of probes and a set of metrics to measure
                  systematic behaviour. We also identified ways in which network
                  architectures can generalize non-systematically, and discuss
                  why such forms of generalization may be unsatisfying. As a
                  case study, we performed a series of experiments in the
                  setting of natural language inference (NLI), demonstrating
                  that some NLU systems achieve high overall performance despite
                  being non-systematic. }
}

@article{henderson2017ethics,
  author          = {{Henderson}, Peter and {Sinha}, Koustuv and
                  {Angelard-Gontier}, Nicolas and {Ke}, {Nan Rosemary} and
                  {Fried}, {Genevieve} and {Lowe}, Ryan and {Pineau}, Joelle},
  title           = {Ethical Challenges in Data-Driven Dialogue Systems},
  journal         = {AAAI/ACM AI Ethics and Society Conference},
  year            = 2018,
  arxiv           = {1711.09050},
  code            = {https://github.com/Breakend/EthicsInDialogue},
  abstract        = {The use of dialogue systems as a medium for human-machine
                  interaction is an increasingly prevalent paradigm. A growing
                  number of dialogue systems use conversation strategies that
                  are learned from large datasets. There are well documented
                  instances where interactions with these system have resulted
                  in biased or even offensive conversations due to the
                  data-driven training process. Here, we highlight potential
                  ethical issues that arise in dialogue systems research,
                  including: implicit biases in data-driven systems, the rise of
                  adversarial examples, potential sources of privacy violations,
                  safety concerns, special considerations for reinforcement
                  learning systems, and reproducibility concerns. We also
                  suggest areas stemming from these issues that deserve further
                  investigation. Through this initial survey, we hope to spur
                  research leading to robust, safe, and ethically sound dialogue
                  systems.}
}

@article{henderson2018adversarial,
  author          = {Peter Henderson and Koustuv Sinha and Rosemary Nan Ke and
                  Joelle Pineau},
  title           = {Adversarial Gain},
  year            = 2018,
  journal         = {Arxiv Pre-print},
  arxiv           = {1811.01302},
  abstract        = {Adversarial examples can be defined as inputs to a model
                  which induce a mistake - where the model output is different
                  than that of an oracle, perhaps in surprising or malicious
                  ways. Original models of adversarial attacks are primarily
                  studied in the context of classification and computer vision
                  tasks. While several attacks have been proposed in natural
                  language processing (NLP) settings, they often vary in
                  defining the parameters of an attack and what a successful
                  attack would look like. The goal of this work is to propose a
                  unifying model of adversarial examples suitable for NLP tasks
                  in both generative and classification settings. We define the
                  notion of adversarial gain: based in control theory, it is a
                  measure of the change in the output of a system relative to
                  the perturbation of the input (caused by the so-called
                  adversary) presented to the learner. This definition, as we
                  show, can be used under different feature spaces and distance
                  conditions to determine attack or defense effectiveness across
                  different intuitive manifolds. This notion of adversarial gain
                  not only provides a useful way for evaluating adversaries and
                  defenses, but can act as a building block for future work in
                  robustness under adversaries due to its rooted nature in
                  stability and manifold theory. }
}

@article{pineau2020improving,
  title           = {Improving Reproducibility in Machine Learning Research (A
                  Report from the NeurIPS 2019 Reproducibility Program)},
  author          = {Pineau, Joelle and Vincent-Lamarre, Philippe and Sinha,
                  Koustuv and Larivi{\`e}re, Vincent and Beygelzimer, Alina and
                  d'Alch{\'e}-Buc, Florence and Fox, Emily and Larochelle, Hugo},
  journal         = {Journal of Machine Learning Research (JMLR)},
  arxiv           = {2003.12206},
  year            = 2020,
  abstract        = {One of the challenges in machine learning research is to
                  ensure that presented and published results are sound and
                  reliable. Reproducibility, that is obtaining similar results
                  as presented in a paper or talk, using the same code and data
                  (when available), is a necessary step to verify the
                  reliability of research findings. Reproducibility is also an
                  important step to promote open and accessible research,
                  thereby allowing the scientific community to quickly integrate
                  new findings and convert ideas to practice. Reproducibility
                  also promotes the use of robust experimental workflows, which
                  potentially reduce unintentional errors. In 2019, the Neural
                  Information Processing Systems (NeurIPS) conference, the
                  premier international conference for research in machine
                  learning, introduced a reproducibility program, designed to
                  improve the standards across the community for how we conduct,
                  communicate, and evaluate machine learning research. The
                  program contained three components: a code submission policy,
                  a community-wide reproducibility challenge, and the inclusion
                  of the Machine Learning Reproducibility checklist as part of
                  the paper submission process. In this paper, we describe each
                  of these components, how it was deployed, as well as what we
                  were able to learn from this initiative.}
}

@article{sinha2018clutrr,
  author          = {Koustuv Sinha and Shagun Sodhani and William L. Hamilton
                  and Joelle Pineau},
  title           = {Compositional Language Understanding with Text-based
                  Relational Reasoning},
  year            = 2018,
  journal         = {Relational Representation Learning Workshop, NIPS},
  arxiv           = {1811.02959},
  code            = {https://github.com/koustuvsinha/clutrr},
  abstract        = {Neural networks for natural language reasoning have largely
                  focused on extractive, fact-based question-answering (QA) and
                  common-sense inference. However, it is also crucial to
                  understand the extent to which neural networks can perform
                  relational reasoning and combinatorial generalization from
                  natural language---abilities that are often obscured by
                  annotation artifacts and the dominance of language modeling in
                  standard QA benchmarks. In this work, we present a novel
                  benchmark dataset for language understanding that isolates
                  performance on relational reasoning. We also present a neural
                  message-passing baseline and show that this model, which
                  incorporates a relational inductive bias, is superior at
                  combinatorial generalization compared to a traditional
                  recurrent neural network approach. }
}

@article{sinha2018hier,
  author          = {{Sinha}, Koustuv and {Dong}, Yue and {Chi-kit Cheung},
                  Jackie and {Ruths}, Derek},
  title           = {A Hierarchical Neural Attention-based Text Classifier},
  journal         = {Empirical Methods of Natural Language Processing (EMNLP)},
  year            = 2018,
  epdf            = {http://www.aclweb.org/anthology/D18-1094},
  abstract        = {Deep neural networks have been displaying superior
                  performance over traditional supervised classifiers in text
                  classification. They learn to extract useful features
                  automatically when sufficient amount of data is presented.
                  However, along with the growth in the number of documents
                  comes the increase in the number of categories, which often
                  results in poor performance of the multiclass classifiers. In
                  this work, we use external knowledge in the form of topic
                  category taxonomies to aide the classification by introducing
                  a deep hierarchical neural attention-based classifier. Our
                  model performs better than or comparable to state-of-the-art
                  hierarchical models at significantly lower computational cost
                  while maintaining high interpretability.}
}

@article{sinha2019clutrr,
  author          = {Koustuv Sinha and Shagun Sodhani and Jin Dong and Joelle
                  Pineau and William L. Hamilton},
  title           = {CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from
                  Text},
  year            = 2019,
  journal         = {Empirical Methods of Natural Language Processing (EMNLP)},
  arxiv           = {1908.06177},
  honors = {Oral},
  code            = {https://github.com/facebookresearch/clutrr},
  abstract        = {The recent success of natural language understanding (NLU)
                  systems has been troubled by results highlighting the failure
                  of these models to generalize in a systematic and robust way.
                  In this work, we introduce a diagnostic benchmark suite, named
                  CLUTRR, to clarify some key issues related to the robustness
                  and systematicity of NLU systems. Motivated by classic work on
                  inductive logic programming, CLUTRR requires that an NLU
                  system infer kinship relations between characters in short
                  stories. Successful performance on this task requires both
                  extracting relationships between entities, as well as
                  inferring the logical rules governing these relationships.
                  CLUTRR allows us to precisely measure a model's ability for
                  systematic generalization by evaluating on held-out
                  combinations of logical rules, and it allows us to evaluate a
                  model's robustness by adding curated noise facts. Our
                  empirical results highlight a substantial performance gap
                  between state-of-the-art NLU models (e.g., BERT and MAC) and a
                  graph neural network model that works directly with symbolic
                  inputs---with the graph-based model exhibiting both stronger
                  generalization and greater robustness. }
}

@article{sinha2020evaluating,
  title           = {Evaluating Logical Generalization in Graph Neural Networks},
  author          = {Sinha, Koustuv and Sodhani, Shagun and Pineau, Joelle and
                  Hamilton, William L},
  journal         = {arXiv preprint arXiv:2003.06560},
  arxiv           = {2003.06560},
  code            = {https://github.com/facebookresearch/graphlog},
  abstract        = {Recent research has highlighted the role of relational
                  inductive biases in building learning agents that can
                  generalize and reason in a compositional manner. However,
                  while relational learning algorithms such as graph neural
                  networks (GNNs) show promise, we do not understand how
                  effectively these approaches can adapt to new tasks. In this
                  work, we study the task of logical generalization using GNNs
                  by designing a benchmark suite grounded in first-order logic.
                  Our benchmark suite, GraphLog, requires that learning
                  algorithms perform rule induction in different synthetic
                  logics, represented as knowledge graphs. GraphLog consists of
                  relation prediction tasks on 57 distinct logical domains. We
                  use GraphLog to evaluate GNNs in three different setups:
                  single-task supervised learning, multi-task pretraining, and
                  continual learning. Unlike previous benchmarks, our approach
                  allows us to precisely control the logical relationship
                  between the different tasks. We find that the ability for
                  models to generalize and adapt is strongly determined by the
                  diversity of the logical rules they encounter during training,
                  and our results highlight new challenges for the design of GNN
                  models.},
  year            = 2020
}

@article{sinha2020maude,
  author          = {Koustuv Sinha and Prasanna Parthasarathi and Jasmine Wang
                  and Ryan Lowe and William L. Hamilton and Joelle Pineau},
  title           = {Learning an Unreferenced Metric for Online Dialogue
                  Evaluation},
  year            = 2020,
  journal         = {Association of Computational Linguistics (ACL)},
  arxiv           = {2005.00583},
  code            = {https://github.com/facebookresearch/online_dialog_eval},
  url             = {https://arxiv.org/abs/2005.00583},
  abstract        = {Evaluating the quality of a dialogue interaction between
                  two agents is a difficult task, especially in open-domain
                  chit-chat style dialogue. There have been recent efforts to
                  develop automatic dialogue evaluation metrics, but most of
                  them do not generalize to unseen datasets and/or need a
                  human-generated reference response during inference, making it
                  infeasible for online evaluation. Here, we propose an
                  unreferenced automated evaluation metric that uses large
                  pre-trained language models to extract latent representations
                  of utterances, and leverages the temporal transitions that
                  exist between them. We show that our model achieves higher
                  correlation with human annotations in an online setting, while
                  not requiring true responses for comparison during inference.}
}

@article{sinha2020neurips,
  author          = {Koustuv Sinha and Joelle Pineau and Jessica Forde and
                  Rosemary Nan Ke and Hugo Larochelle},
  title           = {NeurIPS 2019 Reproducibility Challenge},
  journal         = {ReScience C},
  year            = 2020,
  volume          = 6,
  number          = 2,
  pages           = {\#11},
  month           = {may},
  doi             = {10.5281/zenodo.3818627},
  url             = {https://doi.org/10.5281/zenodo.3818627},
  epdf            = {../assets/files/rc_neurips_2019.pdf},
  code            = {https://github.com/ReScience/NeurIPS-2019}
}

@article{sodhani2020ideas,
  title           = {Ideas for Improving the Field of Machine Learning:
                  Summarizing Discussion from the NeurIPS 2019 Retrospectives
                  Workshop},
  author          = {Sodhani, Shagun and Jaiswal, Mayoore S and Baker, Lauren
                  and Sinha, Koustuv and Shneider, Carl and Henderson, Peter and
                  Lehman, Joel and Lowe, Ryan},
  journal         = {arXiv preprint arXiv:2007.10546},
  year            = 2020,
  arxiv           = {2007.10546},
  abstract        = {This report documents ideas for improving the field of
                  machine learning, which arose from discussions at the ML
                  Retrospectives workshop at NeurIPS 2019. The goal of the
                  report is to disseminate these ideas more broadly, and in turn
                  encourage continuing discussion about how the field could
                  improve along these axes. We focus on topics that were most
                  discussed at the workshop: incentives for encouraging
                  alternate forms of scholarship, re-structuring the review
                  process, participation from academia and industry, and how we
                  might better train computer scientists as scientists.},
  url             = {https://arxiv.org/abs/2007.10546}
}

@misc{
sharma2021evaluating,
  title           = {Evaluating Gender Bias in Natural Language Inference },
  author          = {Sharma, Shanya and Dey, Manan and Sinha, Koustuv},
  year            = 2021,
  abstract        = {Gender-bias stereotypes have recently raised significant
                  ethical concerns in natural language processing. However,
                  progress in the detection and evaluation of gender-bias in
                  natural language understanding through inference is limited
                  and requires further investigation. In this work, we propose
                  an evaluation methodology to measure these biases by
                  constructing a probe task that involves pairing a
                  gender-neutral premise against a gender-specific hypothesis.
                  We use our probe task to investigate state-of-the-art NLI
                  models on the presence of gender stereotypes using
                  occupations. Our findings suggest that three models (BERT,
                  RoBERTa, and BART) trained on MNLI and SNLI data-sets are
                  significantly prone to gender-induced prediction errors. We
                  also find that debiasing techniques such as augmenting the
                  training dataset to ensure that it is a gender-balanced
                  dataset can help reduce such bias in certain cases.},
  url             = {https://openreview.net/forum?id=bnuU0PzXl0-}
}

@article{sinha2021masked,
  title           = {Masked Language Modeling and the Distributional Hypothesis:
                  Order Word Matters Pre-training for Little},
  author          = {Koustuv Sinha and Robin Jia and Dieuwke Hupkes and Joelle
                  Pineau and Adina Williams and Douwe Kiela},
  abstract        = {A possible explanation for the impressive performance of
                  masked language model (MLM) pre-training is that such models
                  have learned to represent the syntactic structures prevalent
                  in classical NLP pipelines. In this paper, we propose a
                  different explanation: MLMs succeed on downstream tasks almost
                  entirely due to their ability to model higher-order word
                  co-occurrence statistics. To demonstrate this, we pre-train
                  MLMs on sentences with randomly shuffled word order, and show
                  that these models still achieve high accuracy after
                  fine-tuning on many downstream tasks -- including on tasks
                  specifically designed to be challenging for models that ignore
                  word order. Our models perform surprisingly well according to
                  some parametric syntactic probes, indicating possible
                  deficiencies in how we test representations for syntactic
                  information. Overall, our results show that purely
                  distributional information largely explains the success of
                  pre-training, and underscore the importance of curating
                  challenging evaluation datasets that require deeper linguistic
                  knowledge.},
  year            = 2021,
  arxiv           = {2104.06644},
  archivePrefix   = {arXiv},
  journal = {Empirical Methods of Natural Language Processing (EMNLP)},
  primaryClass    = {cs.CL},
  code = {https://github.com/pytorch/fairseq/tree/master/examples/shuffled_word_order}
}

@article{parthasarathi2021want,
      title={Sometimes We Want Translationese},
      author={Prasanna Parthasarathi and Koustuv Sinha and Joelle Pineau and Adina Williams},
      abstract={Rapid progress in Neural Machine Translation (NMT) systems over the last few years has been driven primarily towards improving translation quality, and as a secondary focus, improved robustness to input perturbations (e.g. spelling and grammatical mistakes). While performance and robustness are important objectives, by over-focusing on these, we risk overlooking other important properties. In this paper, we draw attention to the fact that for some applications, faithfulness to the original (input) text is important to preserve, even if it means introducing unusual language patterns in the (output) translation. We propose a simple, novel way to quantify whether an NMT system exhibits robustness and faithfulness, focusing on the case of word-order perturbations. We explore a suite of functions to perturb the word order of source sentences without deleting or injecting tokens, and measure the effects on the target side in terms of both robustness and faithfulness. Across several experimental conditions, we observe a strong tendency towards robustness rather than faithfulness. These results allow us to better understand the trade-off between faithfulness and robustness in NMT, and opens up the possibility of developing systems where users have more autonomy and control in selecting which property is best suited for their use case. },
      year={2021},
      arxiv={2104.07623},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      journal={Empirical Methods of Natural Language Processing (EMNLP) Findings}
}

@article{sinha2021unnat,
  title           = {{UnNatural} Language Inference},
  author          = {Koustuv Sinha and Prasanna Parthasarathi and Joelle Pineau
                  and Adina Williams},
  abstract        = {Natural Language Understanding has witnessed a watershed
                  moment with the introduction of large pre-trained Transformer
                  networks. These models achieve state-of-the-art on various
                  tasks, notably including Natural Language Inference (NLI).
                  Many studies have shown that the large representation space
                  imbibed by the models encodes some syntactic and semantic
                  information. However, to really ``know syntax'', a model must
                  recognize when its input violates syntactic rules and
                  calculate inferences accordingly. In this work, we find that
                  state-of-the-art NLI models, such as RoBERTa and BART are
                  invariant to, and sometimes even perform better on, examples
                  with randomly reordered words. With iterative search, we are
                  able to construct randomized versions of NLI test sets, which
                  contain permuted hypothesis-premise pairs with the same words
                  as the original, yet are classified with perfect accuracy by
                  large pre-trained models, as well as pre-Transformer
                  state-of-the-art encoders. We find the issue to be language
                  and model invariant, and hence investigate the root cause. To
                  partially alleviate this effect, we propose a simple training
                  methodology. Our findings call into question the idea that our
                  natural language understanding models, and the tasks used for
                  measuring their progress, genuinely require a human-like
                  understanding of syntax. },
  journal = {Association of Computational Linguistics (ACL)},
  year            = 2021,
  url             = {https://arxiv.org/abs/2101.00010},
  arxiv           = {2101.00010},
  code = {https://github.com/facebookresearch/unlu},
  honors = {Oral, Outstanding Paper Award}
}


@misc{sriram2021covid19,
  title           = {COVID-19 Deterioration Prediction via Self-Supervised
                  Representation Learning and Multi-Image Prediction},
  author          = {Anuroop Sriram and Matthew Muckley and Koustuv Sinha and
                  Farah Shamout and Joelle Pineau and Krzysztof J. Geras and Lea
                  Azour and Yindalon Aphinyanaphongs and Nafissa Yakubova and
                  William Moore},
  abstract        = {The rapid spread of COVID-19 cases in recent months has
                  strained hospital resources, making rapid and accurate triage
                  of patients presenting to emergency departments a necessity.
                  Machine learning techniques using clinical data such as chest
                  X-rays have been used to predict which patients are most at
                  risk of deterioration. We consider the task of predicting two
                  types of patient deterioration based on chest X-rays: adverse
                  event deterioration (i.e., transfer to the intensive care
                  unit, intubation, or mortality) and increased oxygen
                  requirements beyond 6 L per day. Due to the relative scarcity
                  of COVID-19 patient data, existing solutions leverage
                  supervised pretraining on related non-COVID images, but this
                  is limited by the differences between the pretraining data and
                  the target COVID-19 patient data. In this paper, we use
                  self-supervised learning based on the momentum contrast (MoCo)
                  method in the pretraining phase to learn more general image
                  representations to use for downstream tasks. We present three
                  results. The first is deterioration prediction from a single
                  image, where our model achieves an area under receiver
                  operating characteristic curve (AUC) of 0.742 for predicting
                  an adverse event within 96 hours (compared to 0.703 with
                  supervised pretraining) and an AUC of 0.765 for predicting
                  oxygen requirements greater than 6 L a day at 24 hours
                  (compared to 0.749 with supervised pretraining). We then
                  propose a new transformer-based architecture that can process
                  sequences of multiple images for prediction and show that this
                  model can achieve an improved AUC of 0.786 for predicting an
                  adverse event at 96 hours and an AUC of 0.848 for predicting
                  mortalities at 96 hours. A small pilot clinical study
                  suggested that the prediction accuracy of our model is
                  comparable to that of experienced radiologists analyzing the
                  same information.},
  year            = 2021,
  arxiv           = {2101.04909},
  archivePrefix   = {arXiv},
  primaryClass    = {cs.CV},
  code            = {https://github.com/facebookresearch/CovidPrognosis}
}
