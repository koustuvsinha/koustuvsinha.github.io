---
---

@string{aps = {American Physical Society,}}

@article{sinha2020evaluating,
  title={Evaluating Logical Generalization in Graph Neural Networks},
  author={Sinha, Koustuv and Sodhani, Shagun and Pineau, Joelle and Hamilton, William L},
  journal={arXiv preprint arXiv:2003.06560},
  arxiv = {2003.06560},
  code = {https://github.com/facebookresearch/graphlog},
  abstract = {Recent research has highlighted the role of relational inductive biases in building learning agents that can generalize and reason in a compositional manner. However, while relational learning algorithms such as graph neural networks (GNNs) show promise, we do not understand how effectively these approaches can adapt to new tasks. In this work, we study the task of logical generalization using GNNs by designing a benchmark suite grounded in first-order logic. Our benchmark suite, GraphLog, requires that learning algorithms perform rule induction in different synthetic logics, represented as knowledge graphs. GraphLog consists of relation prediction tasks on 57 distinct logical domains. We use GraphLog to evaluate GNNs in three different setups: single-task supervised learning, multi-task pretraining, and continual learning. Unlike previous benchmarks, our approach allows us to precisely control the logical relationship between the different tasks. We find that the ability for models to generalize and adapt is strongly determined by the diversity of the logical rules they encounter during training, and our results highlight new challenges for the design of GNN models.},
  year={2020}
}

@article{sinha2020maude,
  Author = {Koustuv Sinha and Prasanna Parthasarathi and Jasmine Wang and Ryan Lowe and William L. Hamilton and Joelle Pineau},
  Title = {Learning an Unreferenced Metric for Online Dialogue Evaluation},
  Year = {2020},
  journal = {ACL},
  arxiv = {2005.00583},
  code = {https://github.com/facebookresearch/online_dialog_eval},
  url = {https://arxiv.org/abs/2005.00583},
  abstract = {Evaluating the quality of a dialogue interaction between two agents is a difficult task, especially in open-domain chit-chat style dialogue. There have been recent efforts to develop automatic dialogue evaluation metrics, but most of them do not generalize to unseen datasets and/or need a human-generated reference response during inference, making it infeasible for online evaluation. Here, we propose an unreferenced automated evaluation metric that uses large pre-trained language models to extract latent representations of utterances, and leverages the temporal transitions that exist between them. We show that our model achieves higher correlation with human annotations in an online setting, while not requiring true responses for comparison during inference.}
}

@article{goodwin2020probing,
  Author = {Emily Goodwin and Koustuv Sinha and Timothy J. O'Donnell},
  Title = {Probing Linguistic Systematicity},
  Year = {2020},
  journal = {ACL},
  arxiv = {2005.04315},
  url = {https://arxiv.org/abs/2005.04315},
  abstract = {Recently, there has been much interest in the question of whether deep natural language understanding models exhibit systematicity; generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear. There is accumulating evidence that neural models often generalize non-systematically. We examined the notion of systematicity from a linguistic perspective, defining a set of probes and a set of metrics to measure systematic behaviour. We also identified ways in which network architectures can generalize non-systematically, and discuss why such forms of generalization may be unsatisfying. As a case study, we performed a series of experiments in the setting of natural language inference (NLI), demonstrating that some NLU systems achieve high overall performance despite being non-systematic. }
}

@article{pineau2020improving,
  title={Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)},
  author={Pineau, Joelle and Vincent-Lamarre, Philippe and Sinha, Koustuv and Larivi{\`e}re, Vincent and Beygelzimer, Alina and d'Alch{\'e}-Buc, Florence and Fox, Emily and Larochelle, Hugo},
  journal={arXiv preprint arXiv:2003.12206},
  arxiv = {2003.12206},
  year={2020},
  abstract={One of the challenges in machine learning research is to ensure that presented and published results are sound and reliable. Reproducibility, that is obtaining similar results as presented in a paper or talk, using the same code and data (when available), is a necessary step to verify the reliability of research findings. Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice. Reproducibility also promotes the use of robust experimental workflows, which potentially reduce unintentional errors. In 2019, the Neural Information Processing Systems (NeurIPS) conference, the premier international conference for research in machine learning, introduced a reproducibility program, designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research. The program contained three components: a code submission policy, a community-wide reproducibility challenge, and the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process. In this paper, we describe each of these components, how it was deployed, as well as what we were able to learn from this initiative.}
}

@article{sinha2020neurips,
  author= {Koustuv Sinha and Joelle Pineau and Jessica Forde and Rosemary Nan Ke and Hugo Larochelle},
  title= {NeurIPS 2019 Reproducibility Challenge},
  journal={ReScience C},
  year={2020},
  volume={6},
  number={2},
  pages={\#11},
  month={may},
  doi={10.5281/zenodo.3818627},
  url={https://doi.org/10.5281/zenodo.3818627},
  epdf={../assets/files/rc_neurips_2019.pdf},
  code={https://github.com/ReScience/NeurIPS-2019}
}

@article{sinha2019clutrr,
  Author = {Koustuv Sinha and Shagun Sodhani and Jin Dong and Joelle Pineau and William L. Hamilton},
  Title = {CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text},
  Year = {2019},
  journal = {Empirical Methods of Natural Language Processing (EMNLP)},
  arxiv = {1908.06177},
  abstract = {The recent success of natural language understanding (NLU) systems has been troubled by results highlighting the failure of these models to generalize in a systematic and robust way. In this work, we introduce a diagnostic benchmark suite, named CLUTRR, to clarify some key issues related to the robustness and systematicity of NLU systems. Motivated by classic work on inductive logic programming, CLUTRR requires that an NLU system infer kinship relations between characters in short stories. Successful performance on this task requires both extracting relationships between entities, as well as inferring the logical rules governing these relationships. CLUTRR allows us to precisely measure a model's ability for systematic generalization by evaluating on held-out combinations of logical rules, and it allows us to evaluate a model's robustness by adding curated noise facts. Our empirical results highlight a substantial performance gap between state-of-the-art NLU models (e.g., BERT and MAC) and a graph neural network model that works directly with symbolic inputs---with the graph-based model exhibiting both stronger generalization and greater robustness. }
}

@article{Pineau:2019,
  author = {Pineau, Joelle and Sinha, Koustuv and Fried, Genevieve and Ke, Rosemary Nan and Larochelle, Hugo},
  title = {{ICLR Reproducibility Challenge 2019}},
  journal = {ReScience C},
  year = {2019},
  month = may,
  volume = {5},
  number = {2},
  pages = {{#5}},
  doi = {10.5281/zenodo.3158244},
  url = {https://zenodo.org/record/3158244/files/article.pdf},
  code_url = {https://github.com/reproducibility-challenge/iclr_2019},
  code_doi = {},
  data_url = {},
  data_doi = {},
  review_url = {https://github.com/ReScience/submissions/issues/5},
  type = {Editorial},
  language = {},
  domain = {},
  epdf = {../assets/files/rc_iclr_2019.pdf},
  keywords = {machine learning, ICLR, reproducibility challenge}
}

@article{sinha2018clutrr,
  Author = {Koustuv Sinha and Shagun Sodhani and William L. Hamilton and Joelle Pineau},
  Title = {Compositional Language Understanding with Text-based Relational Reasoning},
  Year = {2018},
  journal = {Relational Representation Learning Workshop, NIPS},
  arxiv = {1811.02959},
  code = {https://github.com/koustuvsinha/clutrr},
  abstract = {Neural networks for natural language reasoning have largely focused on extractive, fact-based question-answering (QA) and common-sense inference. However, it is also crucial to understand the extent to which neural networks can perform relational reasoning and combinatorial generalization from natural language---abilities that are often obscured by annotation artifacts and the dominance of language modeling in standard QA benchmarks. In this work, we present a novel benchmark dataset for language understanding that isolates performance on relational reasoning. We also present a neural message-passing baseline and show that this model, which incorporates a relational inductive bias, is superior at combinatorial generalization compared to a traditional recurrent neural network approach. }
}

@article{henderson2018adversarial,
  Author = {Peter Henderson and Koustuv Sinha and Rosemary Nan Ke and Joelle Pineau},
  Title = {Adversarial Gain},
  Year = {2018},
  journal = {Arxiv Pre-print},
  arxiv = {1811.01302},
  abstract = {Adversarial examples can be defined as inputs to a model which induce a mistake - where the model output is different than that of an oracle, perhaps in surprising or malicious ways. Original models of adversarial attacks are primarily studied in the context of classification and computer vision tasks. While several attacks have been proposed in natural language processing (NLP) settings, they often vary in defining the parameters of an attack and what a successful attack would look like. The goal of this work is to propose a unifying model of adversarial examples suitable for NLP tasks in both generative and classification settings. We define the notion of adversarial gain: based in control theory, it is a measure of the change in the output of a system relative to the perturbation of the input (caused by the so-called adversary) presented to the learner. This definition, as we show, can be used under different feature spaces and distance conditions to determine attack or defense effectiveness across different intuitive manifolds. This notion of adversarial gain not only provides a useful way for evaluating adversaries and defenses, but can act as a building block for future work in robustness under adversaries due to its rooted nature in stability and manifold theory. }
}

@article{gontier2018,
  Author = {Nicolas A. Gontier and Koustuv Sinha and Peter Henderson and Iulian Serban and Michael Noseworthy and Prasanna Parthasarathi and Joelle Pineau},
  Title = {The RLLChatbot: a solution to the ConvAI Challenge},
  Year = {2018},
  arxiv = {1811.02714},
  journal = {under review at Dialog & Discourse Journal (D&D)},
  abstract = {Current conversational systems can follow simple commands and answer basic questions, but they have difficulty maintaining coherent and open-ended conversations about specific topics. Competitions like the Conversational Intelligence (ConvAI) challenge are being organized to push the research development towards that goal. This article presents in detail the RLLChatbot that participated in the 2017 ConvAI challenge. The goal of this research is to better understand how current deep learning and reinforcement learning tools can be used to build a robust yet flexible open domain conversational agent. We provide a thorough description of how a dialog system can be built and trained from mostly public-domain datasets using an ensemble model. The first contribution of this work is a detailed description and analysis of different text generation models in addition to novel message ranking and selection methods. Moreover, a new open-source conversational dataset is presented. Training on this data significantly improves the Recall@k score of the ranking and selection mechanisms compared to our baseline model responsible for selecting the message returned at each interaction. }
}

@article{sinha2018hier,
  Author = {{Sinha}, Koustuv and {Dong}, Yue and {Chi-kit Cheung}, Jackie and {Ruths}, Derek},
  Title = {A Hierarchical Neural Attention-based Text Classifier},
  journal = {Empirical Methods of Natural Language Processing (EMNLP)},
  year = {2018},
  epdf = {http://www.aclweb.org/anthology/D18-1094},
  abstract = {Deep neural networks have been displaying superior  performance over traditional supervised classifiers in text classification. They learn to extract useful features automatically when sufficient amount of data is presented. However, along with the growth in the number of documents comes the increase in the number of categories, which often results in poor performance of the multiclass classifiers. In this work, we use external knowledge in the form of topic category taxonomies to aide the classification by introducing a deep hierarchical neural attention-based classifier.  Our model performs better than or comparable to state-of-the-art hierarchical models at significantly lower computational cost while maintaining high interpretability.}
}

@article{henderson2017ethics,
  Author = {{Henderson}, Peter and {Sinha}, Koustuv and {Angelard-Gontier}, Nicolas and {Ke}, {Nan Rosemary} and {Fried}, {Genevieve} and {Lowe}, Ryan and {Pineau}, Joelle},
  Title = {Ethical Challenges in Data-Driven Dialogue Systems},
  journal = {AAAI/ACM AI Ethics and Society Conference},
  year = {2018},
  arxiv = {1711.09050},
  code = {https://github.com/Breakend/EthicsInDialogue},
  abstract = {The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.}
  }
