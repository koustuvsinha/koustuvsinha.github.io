---
layout: post
date: 2021-04-15 9:00:00-0400
inline: true
---

Announcing the pre-print of our paper ["Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little"](https://arxiv.org/abs/2104.06644). We find RoBERTa trained with sentence word order shuffled data performs remarkably close to natural word order pre-trained models on several downstream and probing tasks!
