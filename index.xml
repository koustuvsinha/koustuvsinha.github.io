<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on A minimal Hugo website</title><link>https://example.org/</link><description>Recent content in Home on A minimal Hugo website</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 23 Dec 2022 17:45:00 -0500</lastBuildDate><atom:link href="https://example.org/index.xml" rel="self" type="application/rss+xml"/><item><title>LLMs can sanitize annotations! Using zero shot relation extraction to fix CLUTRR templates</title><link>https://example.org/post/zero_shot_clutrr/</link><pubDate>Fri, 23 Dec 2022 17:45:00 -0500</pubDate><guid>https://example.org/post/zero_shot_clutrr/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>It has been three years since the release of &lt;a href="https://example.org/introducing-clutrr/">CLUTRR&lt;/a>, a benchmark we created to test the reasoning capabilities of modern neural networks. The idea is simple: can models understand first-order logic, in the backdrop of kinship relations? Specifically, we test the ability of the models to perform &lt;em>implicit&lt;/em> reasoning - figuring out the relation of two characters in a given story, where their relation is not provided explicitly. For example, consider the following story:&lt;/p></description></item><item><title>Replicating Zotero-connector functionality in Emacs … without Zotero!</title><link>https://example.org/post/emacs_org_protocol_arxiv/</link><pubDate>Wed, 12 Oct 2022 18:26:00 -0400</pubDate><guid>https://example.org/post/emacs_org_protocol_arxiv/</guid><description>&lt;p>In my &lt;a href="https://example.org/post/emacs_research_workflow/">last blog post&lt;/a> I described a method I use to keep track of my paper reading habits, using Emacs. Using the workflow, I can now:&lt;/p>
&lt;ul>
&lt;li>Check the latest Arxiv papers using &lt;a href="https://github.com/skeeto/elfeed">Elfeed&lt;/a>&lt;/li>
&lt;li>Score the papers using &lt;a href="https://github.com/sp1ff/elfeed-score">Elfeed-Score&lt;/a>&lt;/li>
&lt;li>Save the papers in a local bib file, along with pdfs, using &lt;a href="https://github.com/jkitchin/org-ref">org-ref&lt;/a> functions&lt;/li>
&lt;li>Maintain a paper reading tracker document in &lt;a href="https://orgmode.org/">Org Mode&lt;/a>, where the workflow automatically adds the paper to read.&lt;/li>
&lt;/ul>
&lt;p>One crucial step I later realised which is missing from this workflow is the ability to store papers from my browser. Typically I do not read Elfeed &lt;em>that&lt;/em> religiously - my main source of papers always has been recommendations from colleagues, Twitter, conference acceptance lists etc. Thus, I need a setup where I can easily save an interesting paper I&amp;rsquo;m reading directly from the browser.&lt;/p></description></item><item><title>A workflow for reading, managing and discovering ML research papers with Emacs</title><link>https://example.org/post/emacs_research_workflow/</link><pubDate>Mon, 18 Jul 2022 00:00:00 +0000</pubDate><guid>https://example.org/post/emacs_research_workflow/</guid><description>&lt;p>Over the last couple of years I have steadily transferred most of my workflows in Emacs (more specifically, Doom Emacs). As they truly say, Emacs is not just an editor, it is an operating system. I think Emacs is not for everyone. It has a very steep learning curve, especially with understanding a new language (elisp) for configuration. Having said that, once you learn how to use Emacs, you unlock insane levels of productivity. It is customizable beyond expectation, and allows one to &amp;ldquo;live&amp;rdquo; within Emacs for most of their daily needs. Emacs has helped me streamline my paper reading habits, which I&amp;rsquo;ll talk in detail in this post. Specifically, I use the following tools from the Emacs ecosystem: &lt;a href="https://orgmode.org/">Org-Mode&lt;/a>, &lt;a href="https://github.com/skeeto/elfeed">Elfeed&lt;/a>, &lt;a href="https://github.com/sp1ff/elfeed-score">Elfeed-score&lt;/a>, &lt;a href="https://github.com/tmalsburg/helm-bibtex">Helm-Bibtex&lt;/a> and &lt;a href="https://github.com/jkitchin/org-ref">Org-ref&lt;/a>.&lt;/p></description></item><item><title>ML Reproducibility Tools and Best Practices</title><link>https://example.org/practices_for_reproducibility/</link><pubDate>Wed, 05 Aug 2020 00:00:00 +0000</pubDate><guid>https://example.org/practices_for_reproducibility/</guid><description>&lt;p>A recurrent challenge in machine learning research is to ensure that the
presented and published results are reliable, robust, and reproducible
[&lt;a href="http://proceedings.mlr.press/v97/bouthillier19a.html">4&lt;/a>,&lt;a href="https://arxiv.org/abs/1711.10337">5&lt;/a>,&lt;a href="https://arxiv.org/abs/1709.06560">6&lt;/a>,&lt;a href="https://arxiv.org/abs/1909.06674">7&lt;/a>].&lt;/p>
&lt;p>Reproducibility, obtaining similar results as presented in a paper using
the same code and data, is necessary to verify the reliability of
research findings. Reproducibility is also an important step to promote
open and accessible research, thereby allowing the scientific community
to quickly integrate new findings and convert ideas to practice.
Reproducibility also promotes the use of robust experimental workflows,
which potentially reduce unintentional errors.&lt;/p></description></item><item><title>GraphLog</title><link>https://example.org/about-graphlog/</link><pubDate>Sat, 25 Apr 2020 00:00:00 +0000</pubDate><guid>https://example.org/about-graphlog/</guid><description>&lt;h2 id="graphlog---suite-of-57-graph-worlds-built-using-first-order-logic">&lt;strong>&lt;code>GraphLog&lt;/code>&lt;/strong> - Suite of 57 graph worlds built using first-order logic&lt;/h2>
&lt;p>&lt;em>Koustuv Sinha, Shagun Sodhani, Joelle Pineau and William L. Hamilton&lt;/em>&lt;/p>
&lt;p>&lt;a href="https://github.com/facebookresearch/graphlog">Code&lt;/a> |
&lt;a href="https://graphlog.readthedocs.io/en/latest/">Docs&lt;/a> |
&lt;a href="https://arxiv.org/abs/2003.06560">Paper&lt;/a> |
&lt;a href="https://www.cs.mcgill.ca/~ksinha4/graphlog/">Home Page&lt;/a> |
&lt;a href="https://www.youtube.com/watch?v=TKEjaA4m4jg">Teaser Talk&lt;/a>&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>A question that we are highly interested in finding an answer to is &lt;em>how
generalizable our learning algorithms are&lt;/em>? Human beings
&lt;a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2F0097-7403.24.4.405">are
incredibly good&lt;/a> at generalization - even at old age, we can &lt;em>learn&lt;/em>
new concepts and &lt;em>apply&lt;/em> them in practice. Critical steps towards
building algorithms that &lt;a href="https://arxiv.org/abs/1604.00289">think like
human beings&lt;/a> include &lt;em>Multitask Learning&lt;/em> - the ability to learn
multiple concepts at once; and &lt;em>Continual Learning&lt;/em> - the ability to
accumulate new knowledge without forgetting the previous knowledge.&lt;/p></description></item><item><title>Introducing CLUTRR</title><link>https://example.org/introducing-clutrr/</link><pubDate>Sat, 07 Sep 2019 00:00:00 +0000</pubDate><guid>https://example.org/introducing-clutrr/</guid><description>&lt;p>&lt;b>C&lt;/b>ompositional &lt;b>L&lt;/b>anguage &lt;b>U&lt;/b>nderstanding with &lt;b>T&lt;/b>ext based &lt;b>R&lt;/b>elational &lt;b>R&lt;/b>easoning&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Question Answering (QA) has recently gained popularity as the major
domain of testing reasoning in text. The literature thus contains a
&lt;a href="https://nlpprogress.com/english/question_answering.html">deluge of Question Answering (QA) datasets&lt;/a> to choose from. These datasets test
the system&amp;rsquo;s ability to extract factual answers from the text. However,
there are growing concerns regarding the ability of Natural Language
Understanding (NLU) models to &lt;strong>generalize&lt;/strong> - both in a &lt;em>systematic&lt;/em> and
&lt;em>robust&lt;/em> way. Adding to that, the recent dominance of large pre-trained
language models (such as BERT, &lt;a href="https://arxiv.org/abs/1810.04805">Devlin et al. 2018&lt;/a>) on many NLU
benchmarks including QA suggests that the primary difficulty in these
datasets are about incorporating the statistics of the language, or the
syntax of the language, rather than pure reasoning.&lt;/p></description></item><item><title/><link>https://example.org/newslist/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.org/newslist/</guid><description>&lt;ul>
&lt;li>[06/11/25] Excited to announce the release of our frontier video understanding model, &lt;a href="https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/">VJEPA-2&lt;/a>! Checkout our model &lt;a href="https://github.com/facebookresearch/vjepa2">code&lt;/a>, &lt;a href="https://huggingface.co/collections/facebook/v-jepa-2-6841bad8413014e185b497a6">weights&lt;/a> and &lt;a href="https://www.linkedin.com/posts/merve-noyan-28b1a113a_video-fine-tuning-support-for-metas-v-jepa-activity-7340747091633156096-C6Or">cool demos&lt;/a>!&lt;/li>
&lt;li>[06/11/25] Happy to release the &lt;a href="https://huggingface.co/spaces/facebook/physical_reasoning_leaderboard">Physical World Reasoning&lt;/a> leaderboard, along with a new dataset, &lt;a href="https://github.com/facebookresearch/minimal_video_pairs">MVPBench&lt;/a> (&lt;a href="https://www.arxiv.org/abs/2506.09987">Paper&lt;/a>) for assessing video understanding capabilities of modern VLMs.&lt;/li>
&lt;li>[01/06/24] Serving as a Senior Area Chair at ACL 2024.&lt;/li>
&lt;li>[16/05/24] Excited to announce our large multimodal language model, &lt;span class="underline">Chameleon&lt;/span>, is &lt;a href="https://arxiv.org/abs/2405.09818">now released on arxiv&lt;/a>! We have also released the &lt;a href="https://github.com/facebookresearch/chameleon">code&lt;/a> and &lt;a href="https://ai.meta.com/resources/models-and-libraries/chameleon-downloads/">model weights&lt;/a>, as per our &lt;a href="https://about.fb.com/news/2024/06/releasing-new-ai-research-models-to-accelerate-innovation-at-scale/">Open Source Strategy&lt;/a>!&lt;/li>
&lt;li>[10/18/23] &lt;a href="https://reproml.org/">MLRC 2023&lt;/a> challenge goes live in partnership with TMLR! Read our &lt;a href="https://reproml.org/blog/announcing_mlrc2023/">blog post&lt;/a> for more information.&lt;/li>
&lt;li>[07/10/23] Excited to share that our paper &lt;a href="https://arxiv.org/abs/2212.08979">&amp;ldquo;Language model acceptability judgements are not always robust to context&amp;rdquo;&lt;/a> has received &lt;strong>&lt;a href="https://2023.aclweb.org/program/best_papers/">Outstanding Paper Award&lt;/a>&lt;/strong> at ACL 2023! Very happy and honored!&lt;/li>
&lt;li>[06/01/23] New paper: &lt;a href="https://arxiv.org/abs/2212.08979">&amp;ldquo;Language model acceptability judgements are not always robust to context&amp;rdquo;&lt;/a>, which is now accepted as a long paper in ACL 2023, Toronto.&lt;/li>
&lt;li>[11/02/22] Successfully defended my PhD! Checkout &lt;a href="https://example.org/phd_thesis/">my thesis here&lt;/a>.&lt;/li>
&lt;li>[08/29/22] Excited to announce a major life event: I&amp;rsquo;m starting today as a Research Scientist (Speech &amp;amp; NLP) in &lt;a href="https://ai.facebook.com/">Meta AI&lt;/a> New York!&lt;/li>
&lt;li>[08/19/22] Happy to announce yet another Machine Learning Reproducibility Challenge, &lt;a href="https://paperswithcode.com/rc2022">the MLRC 2022&lt;/a>! This is our six edition!&lt;/li>
&lt;li>[01/10/21] Happy to update that our paper &lt;a href="https://arxiv.org/abs/2104.06644">&amp;ldquo;Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little&amp;rdquo;&lt;/a> is accepted as a long paper at EMNLP 2021!&lt;/li>
&lt;li>[01/09/21] Happy to announce the new iteration of &lt;a href="https://paperswithcode.com/rc2021">ML Reproducibility Challenge 2021&lt;/a>, which has now enlarged to cover 9 top ML conferences! Submit your reports through Feb 2022!&lt;/li>
&lt;li>[03/07/21] On a personal news, got married to my sweetheart &lt;a href="https://atrayeebasu.github.io/">Atrayee&lt;/a> this July!&lt;/li>
&lt;li>[02/07/21] Thrilled to share that our paper &lt;a href="https://arxiv.org/abs/2101.00010">UnNatural Language Inference&lt;/a> has received &lt;strong>Outstanding Paper Award&lt;/strong> at ACL 2021! Deeply honored!&lt;/li>
&lt;li>[15/04/21] Announcing the pre-print of our paper &lt;a href="https://arxiv.org/abs/2104.06644">&amp;ldquo;Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little&amp;rdquo;&lt;/a>. We find RoBERTa trained with sentence word order shuffled data performs remarkably close to natural word order pre-trained models on several downstream and probing tasks!&lt;/li>
&lt;li>[01/06/21] Excited to announce that our paper &lt;a href="https://arxiv.org/abs/2101.00010">&amp;ldquo;UnNatural Language Inference&amp;rdquo;&lt;/a>, has been accepted to ACL 2021 (Long paper, Oral), where we stumble upon the weird language understanding mechanisms employed by NLU models!&lt;/li>
&lt;li>[02/10/20] Happy to announce our paper &lt;a href="https://arxiv.org/abs/2009.14786">&amp;ldquo;Measuring Systematic Generalization in Neural Proof Generation with Transformers&amp;rdquo;&lt;/a> is accepted at NeurIPS 2020!&lt;/li>
&lt;li>[05/09/20] Excited to announce the &lt;a href="https://paperswithcode.com/rc2020">2020 edition of the ML Reproducibility Challenge&lt;/a>! We now cover 7 major ML conferences, do check it out!&lt;/li>
&lt;li>[05/08/20] We released a new blog post on &lt;a href="https://www.cs.mcgill.ca/~ksinha4/practices_for_reproducibility/">ML Reproducibility Tools and Best Practices&lt;/a>. Check it out!&lt;/li>
&lt;li>[30/04/20] Public release of our new multi-task graph dataset, &lt;strong>&lt;code>GraphLog&lt;/code>&lt;/strong>. Check out the &lt;a href="https://www.cs.mcgill.ca/~ksinha4/about-graphlog/">blog post&lt;/a> for more information.&lt;/li>
&lt;li>[08/04/20] Report on &lt;a href="https://arxiv.org/abs/2003.12206">NeurIPS 2019 Reproducibility Program&lt;/a> published on arxiv. We have also published our thoughts on &lt;a href="https://medium.com/@NeurIPSConf/designing-the-reproducibility-program-for-neurips-2020-7fcccaa5c6ad">Designing the Reproducibility Program&lt;/a> for NeurIPS 2020 on Medium.&lt;/li>
&lt;li>[15/04/20] Excited to announce two papers accepted to ACL 2020! &lt;a href="https://arxiv.org/abs/2005.04315">Probing Linguistic Systematicity&lt;/a> and &lt;a href="https://arxiv.org/abs/2005.00583">Learning an unreferenced metric for online Dialog evaluation&lt;/a>.&lt;/li>
&lt;li>[01/12/19] Co-organizing NeurIPS 2019 &lt;a href="https://ml-retrospectives.github.io/neurips2019/">ML Retrospectives Workshop&lt;/a>&lt;/li>
&lt;li>[01/09/19] Co-organizing &lt;a href="https://reproducibility-challenge.github.io/neurips2019/">NeurIPS 2019 Reproducibility Challenge&lt;/a> and honored to be the NeurIPS 2019 Reproducibility Co-Chair.&lt;/li>
&lt;li>[28/01/19] Excited to join Facebook AI Research (FAIR) as PhD Intern!&lt;/li>
&lt;li>[14/08/19] Our paper &lt;em>&lt;a href="https://www.cs.mcgill.ca/~ksinha4/clutrr/">CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text&lt;/a>&lt;/em> accepted at EMNLP 2019!&lt;/li>
&lt;li>[28/09/18] Co-organizing &lt;a href="https://reproducibility-challenge.github.io/iclr_2019/">ICLR Reproducibility Challenge&lt;/a>, 2019&lt;/li>
&lt;li>[04/09/18] Starting PhD at &lt;a href="https://www.cs.mcgill.ca/">McGill University&lt;/a>, advised by Dr &lt;a href="https://www.cs.mcgill.ca/~jpineau/">Joelle Pineau&lt;/a> and Dr &lt;a href="https://www.cs.mcgill.ca/~wlh/">William L. Hamilton&lt;/a>, from Fall 2018.&lt;/li>
&lt;li>[31/08/18] Our paper on &lt;em>A Hierarchical Neural Attention-based Text Classifier&lt;/em> accepted at EMNLP 2018!&lt;/li>
&lt;li>[01/06/18] Intern-ing at &lt;a href="https://www.sait.samsung.co.kr/saithome/main/main.do">Samsung Advanced Institute of Technology&lt;/a> for the Summer!&lt;/li>
&lt;li>[01/02/18] &lt;a href="https://breakend.github.io/EthicsInDialogue/">Our paper&lt;/a> on &lt;em>Ethics in Data Driven Dialog Systems&lt;/em> accepted at AAAI/ACM conference on Ethics &amp;amp; Safety.&lt;/li>
&lt;/ul></description></item></channel></rss>