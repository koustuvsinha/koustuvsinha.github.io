<!doctype html><html lang=en-us><head><meta name=generator content="Hugo 0.148.0"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Home | A minimal Hugo website</title><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/fonts.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/default.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/flexboxgrid/6.3.1/flexboxgrid.min.css integrity="sha512-YHuwZabI2zi0k7c9vtg8dK/63QB0hLvD4thw44dFo/TfBFVVQOqEG9WpviaEpbyvgOIYLXF1n7xDUfU3GDs0sw==" crossorigin=anonymous referrerpolicy=no-referrer></head><body><nav><ul class=menu><li><a href=/>Home</a></li><li><a href=/phd_thesis/>PhD Thesis</a></li><li><a href=/post/>Blog</a></li></ul><hr></nav><img src=/prof_pic_hu_598961ae497bf60.jpg width=200 height=195 alt="Profile pic" class=avatar><h1 id=koustuv-sinha>Koustuv Sinha</h1><p>I&rsquo;m a Research Scientist at <a href=https://ai.meta.com/>Meta AI</a>, in the Fundamental AI Research (FAIR) team. I received my PhD from <a href=http://mcgill.ca/>McGill University</a> (<a href=http://cs.mcgill.ca>School of Computer Science</a>) and <a href=https://mila.quebec>Mila (Quebec AI Institute)</a>, Montreal, Canada, where I was advised by Dr. <a href=https://www.cs.mcgill.ca/~jpineau/>Joelle Pineau</a>. I also received by MSc (Thesis) from McGill University, where I was advised by Dr. <a href=https://derekruths.com/>Derek Ruths</a> and Dr. Joelle Pineau.</p><p>My current research interest involves investigating the role of multimodal language models at understanding and reasoning the world through rich visual representations, especially by leveraging language as a tool to decode and reason the underlying physical rules governing the world. My research also involves understanding the limits of <a href=https://slideslive.com/38922304/from-system-1-deep-learning-to-system-2-deep-learning>systematic</a> language understanding - the ability of neural systems to understand language in a human-like way - by evaluating the extent of their capabilities in understanding semantics, syntax and generalizability.</p><p>I am also involved in improving and enabling reproducibile research in Machine Learning - I&rsquo;m the lead organizer of the annual <em>Machine Learning Reproducibility Challenge</em> (<a href=https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html>V1</a>, <a href=https://www.cs.mcgill.ca/~jpineau/ICLR2019-ReproducibilityChallenge.html>V2</a>, <a href=https://reproducibility-challenge.github.io/neurips2019/>V3</a>, <a href=https://paperswithcode.com/rc2020>V4</a>, <a href=https://paperswithcode.com/rc2021>V5</a>, <a href=https://paperswithcode.com/rc2022>V6</a>, <a href=https://reproml.org/>v7</a>), and I serve as an associate editor at <a href=http://rescience.github.io/>ReScience C</a>, a peer reviewed journal promoting reproducible research. My work has been covered by several news outlets in the past, including <a href=https://www.nature.com/articles/d41586-019-03895-5>Nature</a>, <a href=https://venturebeat.com/2021/01/15/facebook-claims-its-ai-can-anticipate-covid-19-outcomes-using-x-rays/>VentureBeat</a>, <a href=https://www.infoq.com/news/2021/03/facebook-covid-prognosis/>InfoQ</a>, <a href=https://www.dailymail.co.uk/sciencetech/article-9153415/Facebook-claims-AI-predict-four-coronavirus-patients-condition-deteriorate.html>DailyMail</a> and <a href=https://tech.hindustantimes.com/tech/news/facebook-wants-to-help-doctors-fight-covid-19-with-ai-and-xrays-71611044405211.html>Hindustan Times</a>.</p><h2 id=featured-publications>Featured Publications</h2><ul><li><a href=https://arxiv.org/abs/2506.09985>V-JEPA 2</a>: Self-Supervised Video Models Enable Understanding, Prediction and Planning; <em>Mido Assran*, Adrien Bardes*, David Fan*, Quentin Garrido*, Russell Howes*, Mojtaba, Komeili*, Matthew Muckley*, Ammar Rizvi*, Claire Roberts*, <strong>Koustuv Sinha*</strong>, Artem Zholus*, Sergio Arnaud*, Abha Gejji*, Ada Martin*, Francois Robert Hogan*, Daniel Dugas*, Piotr Bojanowski, Vasil Khalidov, Patrick Labatut, Francisco Massa, Marc Szafraniec, Kapil Krishnakumar, Yong Li, Xiaodong Ma, Sarath Chandar, Franziska Meier*, Yann LeCun*, Michael Rabbat*, Nicolas Ballas*</em>; <a href=https://huggingface.co/collections/facebook/v-jepa-2-6841bad8413014e185b497a6>Huggingface</a> | <a href=https://github.com/facebookresearch/vjepa2>Code</a> | <a href=https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/>Blog</a></li><li><a href=https://arxiv.org/abs/2504.01017>Scaling Language-Free Visual Representation Learning</a>; <em>David Fan*, Shengbang Tong*, Jiachen Zhu, <strong>Koustuv Sinha</strong>, Zhuang Liu, Xinlei Chen, Michael Rabbat, Nicolas Ballas, Yann LeCun, Amir Bar, Saining Xie</em>; <a href=https://huggingface.co/collections/facebook/web-ssl-68094132c15fbd7808d1e9bb>Huggingface</a> | <a href=https://github.com/facebookresearch/webssl>Code</a> | <a href=https://davidfan.io/webssl/>Project Page</a></li><li><a href=https://arxiv.org/abs/2412.14164>MetaMorph</a>: Multimodal Understanding and Generation via Instruction Tuning; <em>Shengbang Tong*, David Fan*, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, <strong>Koustuv Sinha</strong>, Michael Rabbat, Yann LeCun, Saining Xie, Zhuang Liu</em></li><li><a href=https://arxiv.org/abs/2410.03478>VEDIT</a>: Latent Prediction Architecture For Procedural Video Representation Learning; <em>Han Lin, Tushar Nagarajan, Nicolas Ballas, Mido Assran, Mojtaba Komeili, Mohit Bansal, <strong>Koustuv Sinha</strong></em>;</li><li><a href=https://arxiv.org/abs/2405.09818>Chameleon</a>: Mixed-Modal Early-Fusion Foundation Models; <em>Chameleon Team</em>; <a href=https://huggingface.co/collections/facebook/chameleon-668da9663f80d483b4c61f58>Huggingface</a> | <a href=https://github.com/facebookresearch/chameleon>Code</a> | <a href=https://ai.meta.com/blog/meta-fair-research-new-releases/>Blog</a></li></ul><p>For a full and up to date list of my publications please visit my <a href="https://scholar.google.ca/citations?hl=en&amp;user=9P9QcckAAAAJ">Google Scholar</a> profile.</p><h2 id=news>News</h2><ul><li>[06/11/25] Excited to announce the release of our frontier video understanding model, <a href=https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/>VJEPA-2</a>! Checkout our model <a href=https://github.com/facebookresearch/vjepa2>code</a>, <a href=https://huggingface.co/collections/facebook/v-jepa-2-6841bad8413014e185b497a6>weights</a> and <a href=https://www.linkedin.com/posts/merve-noyan-28b1a113a_video-fine-tuning-support-for-metas-v-jepa-activity-7340747091633156096-C6Or>cool demos</a>!</li><li>[06/11/25] Happy to release the <a href=https://huggingface.co/spaces/facebook/physical_reasoning_leaderboard>Physical World Reasoning</a> leaderboard, along with a new dataset, <a href=https://github.com/facebookresearch/minimal_video_pairs>MVPBench</a> (<a href=https://www.arxiv.org/abs/2506.09987>Paper</a>) for assessing video understanding capabilities of modern VLMs.</li><li>[01/06/24] Serving as a Senior Area Chair at ACL 2024.</li><li>[16/05/24] Excited to announce our large multimodal language model, <span class=underline>Chameleon</span>, is <a href=https://arxiv.org/abs/2405.09818>now released on arxiv</a>! We have also released the <a href=https://github.com/facebookresearch/chameleon>code</a> and <a href=https://ai.meta.com/resources/models-and-libraries/chameleon-downloads/>model weights</a>, as per our <a href=https://about.fb.com/news/2024/06/releasing-new-ai-research-models-to-accelerate-innovation-at-scale/>Open Source Strategy</a>!</li></ul><h2 id=interns>Interns</h2><p>I&rsquo;m fortunate to be supervising / have supervised exceptionally strong interns, and always looking to support more students!</p><ul><li><a href=https://tsb0601.github.io/petertongsb/>Peter Tong</a>, Research Intern @ FAIR, Meta AI 2025-26; Peter also worked with me before, co-supervised with <a href=https://ai.meta.com/people/1148536089838617/michael-rabbat/>Mike Rabbat</a> and <a href=https://liuzhuang13.github.io/>Zhuang Liu</a> as a Research Intern @ Meta AI, 2024</li><li><a href=https://weijiashi.notion.site/>Weijia Shi</a>, Research Intern, FAIR, Meta AI 2025</li><li><a href=https://jiachenzhu.github.io/>Jiachen Zhu</a>, Visiting Researcher, FAIR, Meta AI 2025</li><li><a href=https://hl-hanlin.github.io/>Han Lin</a>, Research Intern @ Meta AI, 2024</li><li><a href=https://bennokrojer.github.io/>Benno Krojer</a>, co-supervised with <a href="https://scholar.google.com/citations?user=euUV4iUAAAAJ&amp;hl=en">Nicolas Ballas</a> and <a href=https://www.midoassran.ca/>Mido Assran</a>, Research Intern @ Meta AI, 2024</li><li><a href=https://jiahuikchen.github.io/>Karen Chen</a>, co-supervised with <a href=https://sites.google.com/site/adriromsor/home>Adriana Romero Soriano</a> and <a href=https://ca.linkedin.com/in/michal-drozdzal-a36b9b42>Michal Drozdal</a>, Research Intern @ Meta AI, 2024</li><li><a href=https://mila.quebec/en/directory/oscar-manas/>Oscar Manas</a>, co-supervised with <a href=https://sites.google.com/site/adriromsor/home>Adriana Romero Soriano</a> and <a href=https://ca.linkedin.com/in/michal-drozdzal-a36b9b42>Michal Drozdal</a>, Research Intern @ Meta AI, 2024</li><li><a href=https://bhargaviparanjape.github.io/>Bhargavi Paranjape</a>, Research Intern @ Meta AI, 2023</li><li><a href=https://meetdavidwan.github.io/>David Wan</a>, co-supervised with <a href=http://www.rama-kanth.com/>Ram Pasunuru</a>, Research Intern @ Meta AI, 2023</li><li><a href=https://songjiang0909.github.io/>Song Jiang</a>, co-supervised with <a href=http://asli.us/>Asli Celikyilmaz</a>, Research Intern @ Meta AI, 2023</li><li><a href=https://www.isi.edu/directory/bremerma/>Jake Bremerman</a>, co-supervised with <a href=https://mingdachen.github.io/>Mingda Chen</a>, Research Intern @ Meta AI, 2023</li><li><a href=https://kumar-shridhar.github.io/>Kumar Shridhar</a>, co-supervised with <a href="https://scholar.google.com/citations?user=lMkTx0EAAAAJ&amp;hl=en">Jason Weston</a>, Research Intern @ Meta AI, 2023</li><li><a href=https://silin159.github.io/SilinGao/>Silin Gao</a>, co-supervised with <a href=https://tianlu-wang.github.io/>Tianlu Wang</a>, Research Intern @ Meta AI, 2023</li><li><a href="https://scholar.google.com/citations?user=padFM5wAAAAJ&amp;hl=en">Saeed Goodarzi</a>, Nikhil Kagita & Dennis Minn, co-supervised with <a href=https://wp.nyu.edu/adinawilliams/>Adina Williams</a>, <a href=https://shtoshni.github.io/>Shubham Toshniwal</a> and <a href=https://www.jacklanchantin.com/>Jack Lanchatin</a>, <a href=https://ds.cs.umass.edu/programs/industry-mentorship-program>UMass Industry Mentorship Program</a> with Meta, Summer of 2023</li></ul><h2 id=invited-talks>Invited Talks</h2><ul><li>Keynote, <a href=https://miccai2023-reproducibility-tutorial.github.io/>Reproducibility Tutorial</a>, MICCAI 2023</li><li>Panelist, <a href=https://ml-eval.github.io/panels/>Reproducibility and Rigor in ML</a>, ML Evaluation Standards Workshop at ICLR 2022, April 2022</li><li>Evaluating Logical Generalization with Graph Neural Networks, Weights and Biases Salon, (<a href="https://www.youtube.com/watch?v=HllTbhy3WSA">Online</a>), May 2020</li><li><em>ML Reproducibility - From Theory to Practice</em>, <a href=https://dl4sci-school.lbl.gov/>DL4Science</a> Seminar, Lawrence Berkeley National Laboratory, Berkeley, (<a href="https://www.youtube.com/watch?v=se7LNICECqI">Online</a>), August 2020; <a href=https://miccai-hackathon.com/>MICCAI Hackathon</a>, Peru, 2020, October 2020; Bielefield University, Germany, hosted by <a href=https://ni.www.techfak.uni-bielefeld.de/people/mschilli>Malte Schilling</a>, October 2021</li></ul><h2 id=featured-awards>Featured Awards</h2><ul><li><strong>Outstanding Paper Award, ACL 2022</strong>, <a href=https://arxiv.org/abs/2212.08979>Language model acceptability judgements are not always robust to context</a>; <em><strong>Koustuv Sinha</strong>, Jon Gauthier, Aaron Mueller, Kanishka Misra, Keren Fuentes, Roger Levy, Adina Williams</em>, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022</li><li><strong>Outstanding Paper Award, ACL 2021</strong>, <a href=https://arxiv.org/abs/2101.00010>UnNatural Language Inference</a>, <em><strong>Koustuv Sinha</strong>, Prasanna Parthasarathi, Joelle Pineau, Adina Williams</em></li></ul><h2 id=academic-responsibilities>Academic Responsibilities</h2><ul><li>Lead organizer, <a href=https://reproml.org/>ML Reproducibility Challenge</a> : (<a href=https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge.html>v1</a>,
<a href=https://www.cs.mcgill.ca/~jpineau/ICLR2019-ReproducibilityChallenge.html>v2</a>,
<a href=https://reproducibility-challenge.github.io/neurips2019/>v3</a>,
<a href=https://paperswithcode.com/rc2020>v4</a>,
<a href=https://paperswithcode.com/rc2021>v5</a>,
<a href=https://paperswithcode.com/rc2022>v6</a>, <a href=https://reproml.org/proceedings/mlrc2023/>v7</a>, <a href=https://reproml.org/call_for_papers/>v8</a>)</li><li>Senior Area Chair: <a href=https://2024.aclweb.org/>ACL 2024</a></li><li>Area Chair: COLM 2025, ACL ARR May 2025, ACL ARR February 2025 (ACL 2025), ACL ARR December 2024, ACL ARR June 2024</li><li>Journal Chair: NeurIPS 2022</li><li>Reproducibility Chair: NeurIPS 2019, NeurIPS 2020</li><li>Reviewer: ACL ARR Cycle Reviewer, ICCV 2025, CVPR 2025, NeurIPS 2024, COLM 2024, and many more &mldr;</li></ul><h2 id=contact>Contact</h2><p>Best place to reach out to me is through email. I&rsquo;m on social media platforms but I rarely monitor them.</p><ul><li><code>{firstname}.{lastname}@{mail.mgill.ca}</code> / <code>{firstnamelastname}@{gmail.com}</code></li><li><a href=https://bsky.app/profile/koustuvsinha.com>BlueSky</a> | <a href=https://twitter.com/koustuvsinha>Twitter/X</a> | <a href=https://www.linkedin.com/>LinkedIn</a> | <a href=https://github.com/koustuvsinha>Github</a></li></ul><ul></ul><footer><link rel=stylesheet href=//cdn.jsdelivr.net/npm/katex/dist/katex.min.css><script src=//cdn.jsdelivr.net/npm/@xiee/utils/js/math-code.min.js defer></script><script src=//cdn.jsdelivr.net/npm/katex/dist/katex.min.js defer></script><script src=//cdn.jsdelivr.net/npm/katex/dist/contrib/auto-render.min.js defer></script><script src=//cdn.jsdelivr.net/npm/@xiee/utils/js/render-katex.js defer></script><script src=//cdn.jsdelivr.net/npm/@xiee/utils/js/center-img.min.js defer></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/languages/python.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/languages/lisp.min.js></script><script>hljs.highlightAll()</script><hr>Â© Koustuv Sinha 2025. Opinions expressed on this website are solely my own. Built using <a href>XMin</a> theme and Hugo.</footer></body></html>